{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Posts Scrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scrape data from the reddit, first of all you need to validate your self."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T03:47:51.287005Z",
     "start_time": "2020-09-02T03:47:50.376733Z"
    }
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T03:47:53.950744Z",
     "start_time": "2020-09-02T03:47:53.945860Z"
    }
   },
   "outputs": [],
   "source": [
    "# chenge this\n",
    "my_client_id = 'Personal_Use_Script'\n",
    "my_client_secret = 'Client_Secret_Key'\n",
    "my_user_agent = 'Reddit_App_Name'\n",
    "\n",
    "my_client_id = 'wN_rkliIFqRoeA'\n",
    "my_client_secret = 'y_wogg5uZgnIguaX7zA3fsa9Luc'\n",
    "my_user_agent = 'reddit_scrapper'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T03:47:54.927676Z",
     "start_time": "2020-09-02T03:47:54.914826Z"
    }
   },
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=my_client_id, \\\n",
    "                     client_secret=my_client_secret, \\\n",
    "                     user_agent=my_user_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T03:47:56.225607Z",
     "start_time": "2020-09-02T03:47:56.219874Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dict = {\"title\":[], \"id\":[], \"redditor\":[], \"num_upvotes\":[], \\\n",
    "             \"subreddit\":[], \"url\":[], \"num_comments\": [],\\\n",
    "             \"created_on\": [], \"body\":[], \"upvote_ratio\":[], \"over_18\":[],\\\n",
    "             \"link_flair_text\":[], \"edited\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T03:47:56.932670Z",
     "start_time": "2020-09-02T03:47:56.923427Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_posts(subreddit_name):\n",
    "    subreddit= reddit.subreddit(subreddit_name).top(limit=1000)\n",
    "    for submission in subreddit:\n",
    "        data_dict[\"title\"].append(submission.title)\n",
    "        data_dict[\"num_upvotes\"].append(submission.score)\n",
    "        data_dict[\"id\"].append(submission.id)\n",
    "        data_dict[\"subreddit\"].append(submission.subreddit)\n",
    "        data_dict[\"url\"].append(submission.url)\n",
    "        data_dict[\"num_comments\"].append(submission.num_comments)\n",
    "        data_dict[\"created_on\"].append(submission.created)\n",
    "        data_dict[\"body\"].append(submission.selftext)\n",
    "        data_dict[\"upvote_ratio\"].append(submission.upvote_ratio)\n",
    "        data_dict[\"over_18\"].append(submission.over_18)\n",
    "        data_dict[\"link_flair_text\"].append(submission.link_flair_text)\n",
    "        data_dict[\"edited\"].append(submission.edited)\n",
    "        data_dict[\"redditor\"].append(submission.author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T03:47:57.712321Z",
     "start_time": "2020-09-02T03:47:57.707225Z"
    }
   },
   "outputs": [],
   "source": [
    "subreddits= ['datascience', 'MachineLearning', 'learnmachinelearning', \n",
    "             'LanguageTechnology', 'deeplearning', 'datasets', 'visualization',\n",
    "             'dataisbeautiful', 'learnpython', 'MLQuestions', 'DataVizRequests',\n",
    "             'statistics', 'SQL', 'neuralnetworks', 'mlpapers', 'MachinesLearn',\n",
    "             'datacleaning', 'artificial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T03:53:10.306809Z",
     "start_time": "2020-09-02T03:47:59.101422Z"
    }
   },
   "outputs": [],
   "source": [
    "for subreddit in subreddits:\n",
    "    get_posts(subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T03:53:11.552192Z",
     "start_time": "2020-09-02T03:53:11.471375Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T03:53:12.754407Z",
     "start_time": "2020-09-02T03:53:12.715114Z"
    }
   },
   "outputs": [],
   "source": [
    "data['created_on'] = data['created_on'].apply(lambda x: dt.datetime.fromtimestamp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T03:53:13.839888Z",
     "start_time": "2020-09-02T03:53:13.798467Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>id</th>\n",
       "      <th>redditor</th>\n",
       "      <th>num_upvotes</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_on</th>\n",
       "      <th>body</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>over_18</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>edited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It’s never too early</td>\n",
       "      <td>fg73za</td>\n",
       "      <td>da_chosen1</td>\n",
       "      <td>2903</td>\n",
       "      <td>datascience</td>\n",
       "      <td>https://i.redd.it/0c9louclfrl41.jpg</td>\n",
       "      <td>58</td>\n",
       "      <td>2020-03-10 16:26:29</td>\n",
       "      <td></td>\n",
       "      <td>0.98</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shout Out to All the Mediocre Data Scientists ...</td>\n",
       "      <td>hohvgq</td>\n",
       "      <td>MrBurritoQuest</td>\n",
       "      <td>2657</td>\n",
       "      <td>datascience</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>264</td>\n",
       "      <td>2020-07-10 17:15:31</td>\n",
       "      <td>I've been lurking on this sub for a while now ...</td>\n",
       "      <td>0.98</td>\n",
       "      <td>False</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Imposter Syndrome is a problem for me and I th...</td>\n",
       "      <td>e6iy5o</td>\n",
       "      <td>ExecutiveFingerblast</td>\n",
       "      <td>2495</td>\n",
       "      <td>datascience</td>\n",
       "      <td>https://i.redd.it/e292g50m4u241.jpg</td>\n",
       "      <td>136</td>\n",
       "      <td>2019-12-06 05:16:05</td>\n",
       "      <td></td>\n",
       "      <td>0.98</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True that</td>\n",
       "      <td>ejvao9</td>\n",
       "      <td>None</td>\n",
       "      <td>2314</td>\n",
       "      <td>datascience</td>\n",
       "      <td>https://i.redd.it/vh0ey1fgsm841.jpg</td>\n",
       "      <td>227</td>\n",
       "      <td>2020-01-05 00:23:50</td>\n",
       "      <td></td>\n",
       "      <td>0.97</td>\n",
       "      <td>False</td>\n",
       "      <td>Fun/Trivia</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Graph of graph analysis</td>\n",
       "      <td>frkgr7</td>\n",
       "      <td>VeryOddEvey</td>\n",
       "      <td>2233</td>\n",
       "      <td>datascience</td>\n",
       "      <td>https://i.redd.it/m99e3svtpqp41.jpg</td>\n",
       "      <td>43</td>\n",
       "      <td>2020-03-30 18:17:40</td>\n",
       "      <td></td>\n",
       "      <td>0.98</td>\n",
       "      <td>False</td>\n",
       "      <td>Fun/Trivia</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15931</th>\n",
       "      <td>Received an unfriendly reminder about the bias...</td>\n",
       "      <td>dd7wi4</td>\n",
       "      <td>varkarrus</td>\n",
       "      <td>52</td>\n",
       "      <td>artificial</td>\n",
       "      <td>https://i.imgur.com/vIGYHbO.png</td>\n",
       "      <td>80</td>\n",
       "      <td>2019-10-05 03:42:26</td>\n",
       "      <td></td>\n",
       "      <td>0.65</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15932</th>\n",
       "      <td>Machine learning finds new metamaterial design...</td>\n",
       "      <td>d9d5kp</td>\n",
       "      <td>chicompj</td>\n",
       "      <td>56</td>\n",
       "      <td>artificial</td>\n",
       "      <td>https://phys.org/news/2019-09-machine-metamate...</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-09-26 15:34:12</td>\n",
       "      <td></td>\n",
       "      <td>0.94</td>\n",
       "      <td>False</td>\n",
       "      <td>news</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15933</th>\n",
       "      <td>AutoML + GAN = AutoGAN! AI Can Now Design Bett...</td>\n",
       "      <td>ctyrew</td>\n",
       "      <td>Yuqing7</td>\n",
       "      <td>53</td>\n",
       "      <td>artificial</td>\n",
       "      <td>https://medium.com/syncedreview/automl-gan-aut...</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-08-23 04:40:47</td>\n",
       "      <td></td>\n",
       "      <td>0.92</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15934</th>\n",
       "      <td>30 AI terms to study (intermediate level)</td>\n",
       "      <td>cmy5nl</td>\n",
       "      <td>reimmoriks</td>\n",
       "      <td>54</td>\n",
       "      <td>artificial</td>\n",
       "      <td>https://lionbridge.ai/articles/30-intermediate...</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-08-07 13:15:30</td>\n",
       "      <td></td>\n",
       "      <td>0.84</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15935</th>\n",
       "      <td>Artificial Intelligence War Between The U.S. A...</td>\n",
       "      <td>cabj69</td>\n",
       "      <td>newworld-ai</td>\n",
       "      <td>53</td>\n",
       "      <td>artificial</td>\n",
       "      <td>https://www.newworldai.com/artificial-intellig...</td>\n",
       "      <td>16</td>\n",
       "      <td>2019-07-08 09:57:54</td>\n",
       "      <td></td>\n",
       "      <td>0.89</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15936 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title      id  \\\n",
       "0                                   It’s never too early  fg73za   \n",
       "1      Shout Out to All the Mediocre Data Scientists ...  hohvgq   \n",
       "2      Imposter Syndrome is a problem for me and I th...  e6iy5o   \n",
       "3                                              True that  ejvao9   \n",
       "4                                Graph of graph analysis  frkgr7   \n",
       "...                                                  ...     ...   \n",
       "15931  Received an unfriendly reminder about the bias...  dd7wi4   \n",
       "15932  Machine learning finds new metamaterial design...  d9d5kp   \n",
       "15933  AutoML + GAN = AutoGAN! AI Can Now Design Bett...  ctyrew   \n",
       "15934          30 AI terms to study (intermediate level)  cmy5nl   \n",
       "15935  Artificial Intelligence War Between The U.S. A...  cabj69   \n",
       "\n",
       "                   redditor  num_upvotes    subreddit  \\\n",
       "0                da_chosen1         2903  datascience   \n",
       "1            MrBurritoQuest         2657  datascience   \n",
       "2      ExecutiveFingerblast         2495  datascience   \n",
       "3                      None         2314  datascience   \n",
       "4               VeryOddEvey         2233  datascience   \n",
       "...                     ...          ...          ...   \n",
       "15931             varkarrus           52   artificial   \n",
       "15932              chicompj           56   artificial   \n",
       "15933               Yuqing7           53   artificial   \n",
       "15934            reimmoriks           54   artificial   \n",
       "15935           newworld-ai           53   artificial   \n",
       "\n",
       "                                                     url  num_comments  \\\n",
       "0                    https://i.redd.it/0c9louclfrl41.jpg            58   \n",
       "1      https://www.reddit.com/r/datascience/comments/...           264   \n",
       "2                    https://i.redd.it/e292g50m4u241.jpg           136   \n",
       "3                    https://i.redd.it/vh0ey1fgsm841.jpg           227   \n",
       "4                    https://i.redd.it/m99e3svtpqp41.jpg            43   \n",
       "...                                                  ...           ...   \n",
       "15931                    https://i.imgur.com/vIGYHbO.png            80   \n",
       "15932  https://phys.org/news/2019-09-machine-metamate...             2   \n",
       "15933  https://medium.com/syncedreview/automl-gan-aut...             4   \n",
       "15934  https://lionbridge.ai/articles/30-intermediate...             0   \n",
       "15935  https://www.newworldai.com/artificial-intellig...            16   \n",
       "\n",
       "               created_on                                               body  \\\n",
       "0     2020-03-10 16:26:29                                                      \n",
       "1     2020-07-10 17:15:31  I've been lurking on this sub for a while now ...   \n",
       "2     2019-12-06 05:16:05                                                      \n",
       "3     2020-01-05 00:23:50                                                      \n",
       "4     2020-03-30 18:17:40                                                      \n",
       "...                   ...                                                ...   \n",
       "15931 2019-10-05 03:42:26                                                      \n",
       "15932 2019-09-26 15:34:12                                                      \n",
       "15933 2019-08-23 04:40:47                                                      \n",
       "15934 2019-08-07 13:15:30                                                      \n",
       "15935 2019-07-08 09:57:54                                                      \n",
       "\n",
       "       upvote_ratio  over_18 link_flair_text edited  \n",
       "0              0.98    False            None  False  \n",
       "1              0.98    False      Discussion  False  \n",
       "2              0.98    False            None  False  \n",
       "3              0.97    False      Fun/Trivia  False  \n",
       "4              0.98    False      Fun/Trivia  False  \n",
       "...             ...      ...             ...    ...  \n",
       "15931          0.65    False            None  False  \n",
       "15932          0.94    False            news  False  \n",
       "15933          0.92    False            None  False  \n",
       "15934          0.84    False            None  False  \n",
       "15935          0.89    False            None  False  \n",
       "\n",
       "[15936 rows x 13 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T03:53:15.317333Z",
     "start_time": "2020-09-02T03:53:14.997773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past reddit posts: (31866, 13)\n"
     ]
    }
   ],
   "source": [
    "reddit_old_data = pd.read_csv(\"machine_learning_and_data_science_subreddit_data.csv\")\n",
    "print(f\"past reddit posts: {reddit_old_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T03:53:16.402541Z",
     "start_time": "2020-09-02T03:53:16.349013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new reddit posts: 15936 past reddit posts: 31866 all reddit posts: 47802\n"
     ]
    }
   ],
   "source": [
    "reddit_all_df = pd.concat([reddit_old_data, data], axis=0)\n",
    "print(f\"new reddit posts: {data.shape[0]} past reddit posts: {reddit_old_data.shape[0]} all reddit posts: {reddit_all_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T03:53:17.923509Z",
     "start_time": "2020-09-02T03:53:17.591971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all reddit posts: (31943, 13)\n"
     ]
    }
   ],
   "source": [
    "reddit_all_df.drop_duplicates(subset = [\"title\", \"id\", \"redditor\", \"subreddit\"], inplace=True)\n",
    "print(f\"all reddit posts: {reddit_all_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T04:04:00.806401Z",
     "start_time": "2020-09-02T04:03:59.853533Z"
    }
   },
   "outputs": [],
   "source": [
    "reddit_all_df.to_csv('machine_learning_and_data_science_subreddit_data.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
