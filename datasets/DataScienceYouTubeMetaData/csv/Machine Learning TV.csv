position,channelId,channelTitle,videoId,publishedAt,publishedAtSQL,videoTitle,videoDescription,videoCategoryId,videoCategoryLabel,duration,durationSec,dimension,definition,caption,thumbnail_maxres,licensedContent,viewCount,likeCount,dislikeCount,favoriteCount,commentCount
1,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,gHHy2w2agEo,2020-07-21T02:35:44Z,21/7/20 2:35,Language Model Evaluation and Perplexity,"Course Link: https://www.coursera.org/lecture/probabilistic-models-in-nlp/language-model-evaluation-SEO4T Transcript: In this video I'll show you how to evaluate a language model. The metric for this is called perplexity and I will explain what this is. First, you'll divide the text corpus into train validation and test data, then you will dive into the concepts of perplexity an important metric used to evaluate language models. So, how can you tell how well your language model is performing? Recall from the previous videos that a language model assigns a probability to each sentence. The model was trained on the corpus. So for the training sentences, it may assign very high probabilities. You should therefore first split the corpus to have some testing and validation data that are not used for the training. As you may have done in the other machine learning projects, you'll create the following splits of training validation and test sets. The training set is used to train your model. The validation set is used for things like tuning hyper-parameters, and the test set is held out for the end. Where you test it once and get an accuracy score that reflects how well your model performs on unseen data.",28,Science & Technology,PT6M46S,406,2d,hd,FALSE,https://i.ytimg.com/vi/gHHy2w2agEo/maxresdefault.jpg,1,375,13,0,0,0
2,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,_z-a6WoNC2s,2020-06-02T12:48:05Z,2/6/20 12:48,"Common Patterns in Time Series: Seasonality, Trend and Autocorrelation","Course link: https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction Time-series come in all shapes and sizes, but there are a number of very common patterns. So it's useful to recognize them when you see them. For the next few minutes we'll take a look at some examples. The first is trend, where time series have a specific direction that they're moving in. As you can see from the Moore's Law example we showed earlier, this is an upwards facing trend. Another concept is seasonality, which is seen when patterns repeat at predictable intervals. For example, take a look at this chart showing active users at a website for software developers. It follows a very distinct pattern of regular dips. Can you guess what they are? Well, what if I told you if it was up for five units and then down for two? Then you could tell that it very clearly dips on the weekends when less people are working and thus it shows seasonality. Other seasonal series could be shopping sites that peak on weekends or sport sites that peak at various times throughout the year, like the draft or opening day, the All-Star day playoffs and maybe the championship game. Of course, some time series can have a combination of both trend and seasonality as this chart shows.",28,Science & Technology,PT5M6S,306,2d,hd,FALSE,https://i.ytimg.com/vi/_z-a6WoNC2s/maxresdefault.jpg,1,529,24,1,0,0
3,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,H6oOhElB3yE,2020-03-20T22:30:41Z,20/3/20 22:30,Limitations of Graph Neural Networks (Stanford University),,28,Science & Technology,PT1H26M35S,1595,2d,hd,FALSE,https://i.ytimg.com/vi/H6oOhElB3yE/maxresdefault.jpg,1,4285,131,0,0,2
4,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,0lpT-yveuIA,2020-02-12T20:30:47Z,12/2/20 20:30,Understanding Metropolis-Hastings algorithm,"Course link: https://www.coursera.org/learn/mcmc-bayesian-statistics Metropolis-Hastings is an algorithm that allows us to sample from a generic probability distribution, which we'll call our target distribution, even if we don't know the normalizing constant. To do this, we construct and sample from a Markov chain whose stationary distribution is the target distribution that we're looking for. It consists of picking an arbitrary starting value and then iteratively accepting or rejecting candidate samples drawn from another distribution, one that is easy to sample. Let's say we want to produce samples from a target distribution. We're going to call it p of theta. But we only know it up to a normalizing constant or up to proportionality. What we have is g of theta. So we don't know the normalizing constant because perhaps this is difficult to integrate. So we only have g of theta to work with. The Metropolis Hastings Algorithm will proceed as follows. The first step is to select an initial value for theta. We're going to call it theta-naught. The next step is for a large number of iterations, so for i from 1 up to some large number m, we're going to repeat the following. The first thing we're going to do is draw a candidate. We'll call that theta-star as our candidate. And we're going to draw this from a proposal distribution. We're going to call the proposal distribution q of theta-star, given the previous iteration's value of theta. We'll take more about this q distribution soon. The next step is to compute the following ratio. We're going to call this alpha. It is this g function evaluated at the candidate divided by the distribution, or the density here of q, evaluated at the candidate given the previous iteration. And all of this will be divided by g evaluated at the old iteration. That divided by q, evaluated at the old iteration. Given the candidate value. If we rearrange this, it'll be g of the candidate times q of the previous value given the candidate divided by g at the previous value. And q evaluated at the candidate, given the previous value.....",28,Science & Technology,PT9M49S,589,2d,hd,FALSE,https://i.ytimg.com/vi/0lpT-yveuIA/maxresdefault.jpg,1,7769,163,7,0,12
5,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,ByeRnmHJ-uk,2020-02-07T11:55:39Z,7/2/20 11:55,Learning to learn: An Introduction to Meta Learning,"Slides PDF: https://drive.google.com/file/d/1DuHyotdwEAEhmuHQWwRosdiVBVGm8uYx/view Abstract: In recent years, high-capacity models, such as deep neural networks, have enabled very powerful machine learning techniques in domains where data is plentiful. However, domains where data is scarce have proven challenging for such methods because high-capacity function approximators critically rely on large datasets for generalization. This can pose a major challenge for domains ranging from supervised medical image processing to reinforcement learning where real-world data collection (e.g., for robots) poses a major logistical challenge. Meta-learning or few-shot learning offers a potential solution to this problem: by learning to learn across data from many previous tasks, few-shot meta-learning algorithms can discover the structure among tasks to enable fast learning of new tasks. The objective of this tutorial is to provide a unified perspective of meta-learning: teaching the audience about modern approaches, describing the conceptual and theoretical principles surrounding these techniques, presenting where these methods have been applied previously, and discussing the fundamental open problems and challenges within the area. We hope that this tutorial is useful for both machine learning researchers whose expertise lies in other areas, while also providing a new perspective to meta-learning researchers. All in all, we aim to provide audience members with the ability to apply meta-learning to their own applications, and develop new meta-learning algorithms and theoretical analyses driven by the current challenges and limitations of existing work. We will provide a unified perspective of how a variety of meta-learning algorithms enable learning from small datasets, an overview of applications where meta-learning can and cannot be easily applied, and a discussion of the outstanding challenges and frontiers of this sub-field.",28,Science & Technology,PT1H27M17S,1637,2d,sd,FALSE,,1,3238,67,1,0,4
6,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,-zq9-6RbKZc,2019-12-20T13:11:20Z,20/12/19 13:11,Page Ranking: Web as a Graph (Stanford University 2019),,28,Science & Technology,PT1H26M56S,1616,2d,hd,FALSE,https://i.ytimg.com/vi/-zq9-6RbKZc/maxresdefault.jpg,1,1578,55,0,0,2
7,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,yFLiiK8c9CU,2019-11-14T17:06:02Z,14/11/19 17:06,Deep Graph Generative Models (Stanford University - 2019),In this video you will learn about the generative models which are applied directly on graph structures. This is a lecture of Stanford University.,28,Science & Technology,PT1H22M31S,1351,2d,hd,FALSE,https://i.ytimg.com/vi/yFLiiK8c9CU/maxresdefault.jpg,1,6728,152,0,0,8
8,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,7JELX6DiUxQ,2019-11-01T11:41:07Z,1/11/19 11:41,Graph Node Embedding Algorithms (Stanford - Fall 2019),"In this video a group of the most recent node embedding algorithms like Word2vec, Deepwalk, NBNE, Random Walk and GraphSAGE are explained by Jure Leskovec. Amazing class!",28,Science & Technology,PT1H25M51S,1551,2d,hd,FALSE,https://i.ytimg.com/vi/7JELX6DiUxQ/maxresdefault.jpg,1,17964,438,2,0,33
9,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,YrhBZUtgG4E,2019-10-03T17:50:14Z,3/10/19 17:50,Graph Representation Learning (Stanford university),Slide link: http://snap.stanford.edu/class/cs224w-2018/handouts/09-node2vec.pdf,28,Science & Technology,PT1H16M53S,1013,2d,hd,FALSE,,1,32090,738,5,0,22
10,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,vyExfvVMk7A,2019-09-16T02:55:14Z,16/9/19 2:55,Understanding Word Embeddings,"Full course link: https://www.coursera.org/learn/intro-to-deep-learning So, the core idea here is that basically you want the words that have similar neighbores , similar contexts, to be similar in this new virtual representation. Now, let's see how we achieve that. But before we do that, let's actually cover some kind of math of how do we represent the words efficiently. So have a word named word. And technically, to fit it into TensorFlow, you'd probably have to represent it as some kind of number. For example, the ID of this word in your dictionary. And basically, the way you usually use this word in your pipeline is you take one-hot vectors, this large size of a dictionary vector that only has one nonzero value. And then push it through some kind of linear models or neural networks, or similar stuff. The only problem is, you're actually doing this thing very inefficiently. So you have this one-hot vector, and then you multiply it by a weight vector, or a weight matrix. It actually, it's actually process, because you have a lot of weights that gets multiplied by zeros. Now, you could actually compute this kind of weighted sum much more efficiently. If you look slightly closer, you could actually write the answer, you could actually write the answer itself without any sums or multiplications....",28,Science & Technology,PT13M22S,802,2d,hd,FALSE,https://i.ytimg.com/vi/vyExfvVMk7A/maxresdefault.jpg,1,3804,38,15,0,4
11,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,V22sLWRZwF0,2019-07-29T18:37:38Z,29/7/19 18:37,Variational Autoencoders - Part 2 ( Modeling a Distribution of Images ),"Let's start with discussing a problem of fitting a distribution P of X into a data-set of points. Why? Well, we have already discussed this problem in week one, when we discussed how to fit a Gaussian to a data-set of points, we discussed it in week two, when we discussed clustering problem, and how we can solve it by fitting the Gaussian mixture model into our data. And also, we discussed probabilistic PC which is kind of an infinite mixture of Gaussians. But now, we will want to return to this question because it turns out that the methods we covered, like Gaussian or Gaussian mixture model on the probabilistic PC, are not enough to capture the complicated objects like images, like natural images. So, you may want to fit your data-set of natural images into a probabilistic distribution, for example, to generate new data. And, if you try to do that with Gaussian mixture model, it will work, but it will not work as well as some more sophisticated models we will discuss this week. And so, in this example for example, we generated some fake celebrity faces by using a generative model, and you can do these kinds of things if you have a probability distribution of your training data, so you can sample new images from this distribution. And also you can, if you have such a model, like P of X, you can also do a kind of Photoshop of the future applications, like here. So you can, with a few brush strokes, you can change a few pixels in your image, and the program will try to recolor everything else, so the picture will stay for the realistic. So, it will change the color of the hair and etc...",28,Science & Technology,PT10M33S,633,2d,hd,FALSE,https://i.ytimg.com/vi/V22sLWRZwF0/maxresdefault.jpg,1,725,19,1,0,0
12,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,LvKt71lE04w,2019-07-24T17:30:01Z,24/7/19 17:30,Variational Autoencoders - Part 1 (Scaling Variational Inference & Unbiased estimates),"Course Link: https://www.coursera.org/learn/bayesian-methods-in-machine-learning Welcome to week five of our course. This week we're going to talk about how to scale Bayesian methods to large data sets. So, even like 10 years ago, people used to think that Bayesian methods are mostly suited for small data sets because first of all, they're expensive, computation expensive. So if you want to do full Bayesian inference on like one million training examples, you are going to face lots of troubles. And second of all, there may not be beneficial anyway in the case of large data because people used to think that the main idea, the main benefit of Bayesian methods is to utilize your model, and to be able to extract as much information as possible from small data set. And if you have free large data set, then you don't need that, you can use any method you want and it will work just fine.But things changed then,Bayesian methods met deep learning, and people started to make some mixture models that has neural networks instead of a probabilistic model.And this is what this week will be about,how to combine neural networks with the Bayesian methods.So we'll discuss that.We'll discuss how to combine these two ideas.We'll see a particular example of variational old encoder, which allows you to generate nice samples,nice images by using neural network which has some probabilistic interpretation.And then, in the second module of Professor Dmitry Vetrov,will tell you about scalable methods for Bayesian neural networks,and about his cutting edge research inthis area that allowed him to compress neural networks by a lot,and then to fight severe over fitting on some complicated data sets.So, to start with,let's discuss a little bit of the concept of estimation being unbiased.We have already touched on that in the previous week, on week four,on Markov Chain Monte Carlo,but let's make our self a little bit more clear here.We'll need that to build unbiased estimates for gradients of some neural networks...",28,Science & Technology,PT6M26S,386,2d,sd,FALSE,,1,1536,23,2,0,1
13,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,6jl9KkmgDIw,2019-06-01T17:23:57Z,1/6/19 17:23,DBSCAN: Part 2,"Hello and welcome. In this video, we'll be covering DB scan. A density-based clustering algorithm which is appropriate to use when examining spatial data. So let's get started. Most of the traditional clustering techniques such as K-Means, hierarchical, and Fuzzy clustering can be used to group data in an unsupervised way. However, when applied to tasks with arbitrary shaped clusters or clusters within clusters, traditional techniques might not be able to achieve good results that is, elements in the same cluster might not share enough similarity or the performance may be poor. Additionally, while partitioning based algorithms such asK-Means may be easy to understand and implement in practice, the algorithm has no notion of outliers that is, all points are assigned to a cluster even if they do not belong in any. In the domain of anomaly detection, this causes problems as anomalous points will be assigned to the same cluster as normal data points. The anomalous points pull the cluster centroid towards them making it harder to classify them as anomalous points. In contrast, density-based clustering locates regions ofhigh density that are separated from one another by regions of low density. Density in this context is defined as the number of points within a specified radius.A specific and very popular type of density-based clustering is DBSCAN.DBSCAN is particularly effective for taskslike class identification on a spatial context.The wonderful attributes of the DBSCAN algorithm is that it canfind out any arbitrary shaped cluster without getting effected by noise.",28,Science & Technology,PT6M58S,418,2d,hd,FALSE,https://i.ytimg.com/vi/6jl9KkmgDIw/maxresdefault.jpg,1,9142,199,10,0,29
14,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,sKRUfsc8zp4,2019-05-13T23:58:28Z,13/5/19 23:58,DBSCAN: Part 1,"In this session, we are going to introduce a density-based clustering algorithm called DBSCAN. DBSCAN is a density-based spatial clustering algorithm introduced by Martin Ester, Hanz-Peter Kriegel's group in KDD 1996. This paper received the highest impact paper award in the conference of KDD of 2014. This paper developed an interesting algorithms that can discover clusters of arbitrary shape. Actually, DBSCAN itself is acronym of density-based spatial clustering of applications with noise.",28,Science & Technology,PT8M21S,501,2d,hd,FALSE,https://i.ytimg.com/vi/sKRUfsc8zp4/maxresdefault.jpg,1,13497,159,6,0,8
15,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,DODphRRL79c,2019-05-08T18:36:22Z,8/5/19 18:36,Gaussian Mixture Models for Clustering,"Now that we provided some background on Gaussian distributions, we can turn to a very important special case of a mixture model, and one that we're going to emphasize quite a lot in this course and in the assignment, and that's called a mixture of Gaussians. And remember that for any one of our image categories, and for any dimension of our observed vector like the blue intensity in that image, we're going to assume a Gaussian distribution to model that random variable. So for example, for forest images, if we just look at the blue intensity, then we might have a Gaussian distribution shown with the green curve here, which is centered about this value 0.42. And I want to mention here that we're actually assuming a Gaussian for the entire three-dimensional vector RGB. And that Gaussian can have correlation structure and it will have correlation structure between these different intensities, because the amount of RGB in an image tends not to be independent, especially within a given image class. But for the sake of illustrations and keeping all the drawings simple, we're just going to look at one dimension like this blue intensity here. But really, in your head, imagine these Gaussians in this higher dimensional space.........",28,Science & Technology,PT12M13S,733,2d,hd,FALSE,https://i.ytimg.com/vi/DODphRRL79c/maxresdefault.jpg,1,16281,274,12,0,12
16,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,uoV1g3i9Qmw,2019-04-09T18:35:53Z,9/4/19 18:35,Understanding Irreducible Error and Bias (By Emily Fox),"Okay, so, we've talked about three different measures of error. And now in this part, we're gonna talk about three different sources of error. And this is gonna lead us into a conversation of the bias variance trade-off. Okay, so when we were forming our prediction, there are three different sources of error. Noise, bias, and variance. And in this part, we're gonna walk through these three different components, at a very high level. At a more intuitive level. And then following this, there are gonna be two optional sections that go into much more formalism and detail about this. But those are optional because we're not requiring that you know this to get through the course. But for those that are interested, we will be providing the formalism behind these notions that I'm presenting now. Let's look at this first term, this noise term. And as we've mentioned many times in this specialization, data are inherently noisy.",28,Science & Technology,PT6M27S,387,2d,hd,FALSE,https://i.ytimg.com/vi/uoV1g3i9Qmw/maxresdefault.jpg,1,2149,54,1,0,1
17,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,TyKzBoEaeEM,2019-03-21T22:19:40Z,21/3/19 22:19,Python Libraries for Machine Learning You Must Know!,"Course Link: https://www.coursera.org/learn/python-machine-learning Okay, now that we've covered a little bit of background on what machine learning is and some of the major types of machine learning problems, there's nothing like getting started with our own machine learning application in Python. And we're going to get started on that right now. So to do that, we're going to make use of several important Python libraries that will support our work. These include scikit-learn, SciPy, NumPy, pandas, and matplotlib. We recommend installing all of these using the Anaconda Python distribution since it comes with all the libraries we'll need in this part of the course. If you have some other existing Python installation, you can install the libraries we'll be using from the command line using pip, like this. The most important library we'll be using for machine learning is called scikit-learn. Scikit-learn is the most widely used Python library for machine learning and it will be the basis for this course .............",28,Science & Technology,PT4M40S,280,2d,hd,FALSE,https://i.ytimg.com/vi/TyKzBoEaeEM/maxresdefault.jpg,1,982,29,0,0,0
18,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,FdBrwaS8_Ts,2019-03-04T16:57:08Z,4/3/19 16:57,Conditional Probability,"Course Link: https://www.coursera.org/learn/probability-intro In this video, we will define marginal, joint, and conditional probabilities. Introduce Bayes’ theorem for calculating conditional probabilities. And generalize the product rule for calc, calculating joint probabilities, regardless of whether the events are dependent or independent. Remember that previously we've talked about the probability of A and B equals probability of A times probability of B rule. And we said that there was a caveat to this rule, that the events had to be independent. So, we'll wrap up the discussion in this video with what do we do when the events are dependent, or if we don't know and cannot check whether the events are independent or not.",28,Science & Technology,PT12M41S,761,2d,hd,FALSE,https://i.ytimg.com/vi/FdBrwaS8_Ts/maxresdefault.jpg,1,495,16,0,0,0
19,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,AcKA-0d8S1g,2019-01-30T14:04:51Z,30/1/19 14:04,Training Tips for Deep Learning (By Intel),"Full Course Link: https://www.coursera.org/learn/intro-practical-deep-learning (An Introduction to Practical Deep Learning) In this lecture we will provide some training tips and tricks that are maybe useful when training deep learning models. We will start with a quick review then discuss overfitting, data augmentation, and end with a discussion on training validation development and testing. In classical machine learning, traditionally you have an image that is end by end, and you engineer a smaller set of K features. These features can be for example the ratio of the length to the height of the object, or the number of circle objects in the image. Then you apply your valid algorithm to learn to associate these patterns of features with an identity, In this case, vehicle. With supervised learning, we should learn algorithms used pre-labled training data to infer a function. Here imagine that you're trying to classify image into five categories. Vehicles, animals, faces, fruits, and chairs using two features, the two axises here. Supervised learning using the mission learning algorithms listed here allows for the determination of decision boundaries, shown here, using pre-labeled data. Here we can see that, while the decision boundaries are not perfect, they do a reasonable good job classifying the data. We'll now talk about overfitting, a common problem in machine learning.",28,Science & Technology,PT6M9S,369,2d,hd,FALSE,https://i.ytimg.com/vi/AcKA-0d8S1g/maxresdefault.jpg,1,718,9,4,0,0
20,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,f9wIElV4s0A,2019-01-18T02:28:39Z,18/1/19 2:28,Finding the Gradient with Newton-Raphson Method,"In this module, we'll start to use the calculus we've done and put it together with vectors in order to start solving equations. In this first video, we'll look at a nice simple case where we just need to find the gradient, or the derivative in order to solve an equation using what's called the Newton-Raphson method. Now, say we've got that distribution of heights again with a mean, an average, mu and width sigma, and we want to fit an equation to that distribution. So, we don't have to after we've fitted it bother about carrying around all the data points, we just have a model with two parameters; a mean and a width. And we can do everything using just the model. And that would be loads faster simpler, I would let us make predictions and so on. So, it would be much, much nicer, but how do we find the right parameters for the model? How do we find the best mu and sigma we can? What we're going to do is, we're going to find some expression for how well the model fits the data, and then look at how that goodness of fit varies, is the fitting parameters mu and sigma vary.",28,Science & Technology,PT8M15S,495,2d,hd,FALSE,https://i.ytimg.com/vi/f9wIElV4s0A/maxresdefault.jpg,1,742,17,0,0,0
21,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,4nqD5TBlOWU,2018-12-16T19:20:01Z,16/12/18 19:20,Why Does Regularization Reduce Overfitting in Deep Neural Networks?,"Why does regularization help with overfitting?Why does it help with reducing variance problems?Let's go through a couple examples to gain some intuition about how it works.So, recall that high bias, high variance.And I just write pictures from our earlier video that looks something like this.Now, let's see a fitting large and deep neural network.I know I haven't drawn this one too large or too deep,unless you think some neural network and this currently overfitting.So you have some cost function like J of W,B equals sum of the losses.So what we did for regularization was add this extra term that penalizes the weight matrices from being too large.So that was the Frobenius norm.So why is it that shrinking the L two norm orthe Frobenius norm or the parameters might cause less overfitting?One piece of intuition is that if youcrank regularisation lambda to be really, really big,they'll be really incentivized to setthe weight matrices W to be reasonably close to zero.So one piece of intuition is maybe it set the weight to be so close to zero fora lot of hidden units that's basically zeroing out a lot of the impact of these hidden units.And if that's the case,then this much simplified neural network becomes a much smaller neural network........",28,Science & Technology,PT7M10S,430,2d,hd,FALSE,https://i.ytimg.com/vi/4nqD5TBlOWU/maxresdefault.jpg,1,1134,15,1,0,0
22,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,EFBDsJt9EM8,2018-12-11T00:44:19Z,11/12/18 0:44,A Gentle Introduction to the Central Limit Theorem (CLT),"This unit is a formal introduction to statistical inference where you will see building blocks from the previous units come together in commonly used statistical inference methods just as confidence, intervals and hypothesis tests. In this until will also introduce the central limit theorem which provides the basis for these methods. Let's start with an example for a survey conducted by the Pew Research Center. The study is titled Young, Underemployed, and Optimistic, Coming of Age, Slowly, in a Tough Economy. Young adults hit hard by the recession. A plurality of the public. 41% believes young adults rather than middle aged or older adults are having the toughest time in today's economy. Tough economic times, altering young adults daily lives and long term plans. While, negative trends in the labor market have been felt most acutely by the youngest workers. Many adults in their late 20s and early 30s have also felt the impact of the weak economy. Among all 18 to 34 year olds, fully half, 49%, say they have taken a job they didn't want just to pay the bills with 24% saying they have taken an unpaid job to gain work experience ....",28,Science & Technology,PT31M53S,1913,2d,hd,FALSE,https://i.ytimg.com/vi/EFBDsJt9EM8/maxresdefault.jpg,1,1204,21,0,0,2
23,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,dEhGM708xUs,2018-12-08T12:07:20Z,8/12/18 12:07,Deep Neural Network Regularization - Part 1,"If you suspect your neural network is over fitting your data. That is you have a high variance problem, one of the first things you should try per probably regularization. The other way to address high variance, is to get more training data that's also quite reliable. But you can't always get more training data, or it could be expensive to get more data. But adding regularization will often help to prevent overfitting, or to reduce the errors in your network. So let's see how regularization works. Let's develop these ideas using logistic regression. Recall that for logistic regression, you try to minimize the cost function J, which is defined as this cost function. Some of your training examples of the losses of the individual predictions in the different examples, where you recall that w and b in the logistic regression, are the parameters. So w is an x-dimensional parameter vector, and b is a real number. And so to add regularization to the logistic regression, what you do is add to it this thing, lambda, which is called the regularization parameter. I'll say more about that in a second. But lambda/2m times the norm of w squared. So here, the norm of w squared is just equal to sum from j equals 1 to nx of wj squared, or this can also be written w transpose w, it's just a square Euclidean norm of the prime to vector w. And this is called L2 regularization.......",28,Science & Technology,PT9M43S,583,2d,hd,FALSE,,1,996,14,1,0,2
24,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,PKuL6eEbKX8,2018-11-25T01:13:03Z,25/11/18 1:13,The No Bullshit Guide to P-value: Introduction + Frequentist Statistics,"If you read articles in the scientific literature, you'll often see people report P-values when they report statistical tests. P-values are widely used, and it's important to understand what they mean. They're also widely criticized, because people often misinterpret p-values. So in this lecture, the goal is to understand what they mean and how to correctly interpret them. When we talk about p-values, the first question we should ask ourselves is why are they so popular in scientific articles? Well, there's a reason for this, and Benjamini expresses it quite nicely. He says in some sense it offers a first line of defense against being fooled by randomness, separating the signal from the noise. So, this is what the p-values allow you to do. When you interpret your data, you might be very likely to interpret data in favor of the hypothesis that you have, even when the effect might be only slightly in the right direction. The risk is that you're fooling yourself. You might be too likely to declare that something is going on, when you're actually looking at random variation in data. So, p-values are one way to prevent you from fooling yourself. P-values tell you how surprising the data is, assuming that there is no effect. And we'll look at all these aspects in more detail. What surprising means, why they're statements about the data, and why they're built on the idea that there is no effect. Now, some people say that p-values are more accurately explained as what you use if you don't know Bayesian statistics yet. In Bayesian statistics, people don't use p-values.",28,Science & Technology,PT20M31S,1231,2d,hd,FALSE,https://i.ytimg.com/vi/PKuL6eEbKX8/maxresdefault.jpg,1,1152,52,0,0,2
25,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,hfqDy508iC8,2018-11-11T18:24:54Z,11/11/18 18:24,Deep Q-Learning in Tensorflow for CartPole - Part 2,"Reinforcement Learning is a type of machine learning that allows you to create AI agents that learn from the environment by interacting with it. Just like how we learn to ride a bicycle, this kind of AI learns by trial and error. As seen in the picture, the brain represents the AI agent, which acts on the environment. After each action, the agent receives the feedback. The feedback consists of the reward and next state of the environment. The reward is usually defined by a human. If we use the analogy of the bicycle, we can define reward as the distance from the original starting point. Usually, training an agent to play an Atari game takes a while (from few hours to a day). So we will make an agent to play a simpler game called CartPole, but using the same idea used in the paper. CartPole is one of the simplest environments in OpenAI gym (a game simulator). As you can see in the animation from the top, the goal of CartPole is to balance a pole connected with one joint on top of a moving cart. Instead of pixel information, there are 4 kinds of information given by the state, such as the angle of the pole and position of the cart. An agent can move the cart by performing a series of actions of 0 or 1 to the cart, pushing it left or right.",28,Science & Technology,PT5M10S,310,2d,hd,FALSE,,1,719,11,0,0,0
26,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,r596XZ5UJQ8,2018-11-10T14:10:38Z,10/11/18 14:10,Deep Q-Learning using Python - Part 1,"Today we’ll learn about Q-Learning. Q-Learning is a value-based Reinforcement Learning algorithm. This video is the first part of a series of video tutorial about Deep Reinforcement Learning. In this video you’ll learn: What Q-Learning is? Some techniques in Deep Q-Learning Stay tuned for the next parts :) Until then, enjoy it!",28,Science & Technology,PT9M14S,554,2d,hd,FALSE,,1,1073,16,2,0,0
27,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,yGrzu9LU15E,2018-10-25T15:26:38Z,25/10/18 15:26,Deep Learning for Handwritten Digit Recognition - Part 3,"As we promised, this is the third part of Deep Learning for Handwritten Digit Recognition series. I hope you enjoy it. Please like and share this video so that you can help other users.",28,Science & Technology,PT6M3S,363,2d,hd,FALSE,,1,15524,156,23,0,20
28,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,oaYsCVtHveQ,2018-10-22T21:56:46Z,22/10/18 21:56,Perplexity: is Our Model Surprised with a Real Text?,"Hey, and welcome back. This is what you have already seen in the end of our previous video. So just to remind, we have some sequences and we are going to predict the probabilities of these sequences. So we learnt that with bigger language model, you can factorize your probability into some terms. So these are the probabilities of the next word, given the previous words. Now, take a moment to see whether everything is okay with the indices on this slide. Well, you can notice that i can be equal to 0 or to k plus 1, and it goes out of range of our sequence. But that's okay because if you remember our previous video, we discussed that we should have some fake tokens in the beginning of the sequence and in the end of the sequence. So this is qual to 0 and to k plus 1 will be exactly these fake tokens. So everything good here. Let us move forward. This is just a generalization. This is n-gram language model. So the only difference here is that the history gets longer. So we condition not only on the previous words but on the whole sequence of n minus 1 previous words. So just take a note to these denotions here. This is just a brief way to show that we have a sequence of n minus one words. Great. We have some intuition how to estimate these probabilities. So you remember that we can just count some n-grams and normalize these counts. But, now, I want to give you some intuition, not only just intuition but mathematical justification...",28,Science & Technology,PT8M6S,486,2d,hd,FALSE,https://i.ytimg.com/vi/oaYsCVtHveQ/maxresdefault.jpg,1,2538,50,4,0,2
29,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,pYRIOGTPRPU,2018-08-26T12:30:01Z,26/8/18 12:30,Understanding Gated Recurrent Unit (GRU) Deep Neural Network,"You've seen how a basic RNN works.In this video, you learn about the Gated Recurrent Unit which is a modification to the RNN hidden layer that makes it much better capturing long range connections and helps a lot with the vanishing gradient problems.Let's take a look.You've already seen the formula for computing the activations at time t of RNN.It's the activation function applied tothe parameter Wa times the activations in the previous time set,the current input and then plus ba. So I'm going to draw this as a picture.So the RNN unit, I'm going to draw as a picture,drawn as a box which inputs a of t-1, the activation for the last time-step.And also inputs xt and these two go together.And after some weights and after this type of linear calculation,if g is a tanh activation function,then after the tanh, it computes the output activation a.And the output activation a(t) might also be passed to say a softener unit or something that could then be used to output yt. So this is maybe a visualization of the RNN unit of the hidden layer of the RNN in terms of a picture.And I want to show you this picture because we're going to use a similar picture to explain the GRU or the Gated Recurrent Unit.Lots of the idea of GRU were due to these two papers respectively by Yu Young Chang, Kagawa, Gaza Hera, Chang Hung Chu and Jose Banjo.And I'm sometimes going to refer to this sentence which we'd seen in the last video to motivate that............",28,Science & Technology,PT17M7S,1027,2d,hd,FALSE,,1,13241,140,12,0,3
30,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,GiyMGBuu45w,2018-08-12T13:43:53Z,12/8/18 13:43,NLP: Understanding the N-gram language models,"Hi, everyone. You are very welcome to week two of our NLP course. And this week is about very core NLP tasks. So we are going to speak about language models first, and then about some models that work with sequences of words, for example, part-of-speech tagging or named-entity recognition. All those tasks are building blocks for NLP applications. And they're very, very useful. So first thing's first. Let's start with language models. Imagine you see some beginning of a sentence, like This is the. How would you continue it? Probably, as a human,you know that This is how sounds nice, or This is did sounds not nice. You have some intuition. So how do you know this? Well, you have written books. You have seen some texts. So that's obvious for you. Can I build similar intuition for computers? Well, we can try. So we can try to estimate probabilities of the next words, given the previous words. But to do this, first of all,we need some data. So let us get some toy corpus. This is a nice toy corpus about the house that Jack built. And let us try to use it to estimate the probability of house, given This is the. So there are four interesting fragments here. And only one of them is exactly what we need. This is the house. So it means that the probability will be one 1 of 4. By c here, I denote the count. So this the count of This is the house,or any other pieces of text. And these pieces of text are n-grams. n-gram is a sequence of n words. So we can speak about 4-grams here. We can also speak about unigrams, bigrams, trigrams, etc. And we can try to choose the best n,and we will speak about it later. But for now, what about bigrams? Can you imagine what happens for bigrams, for example, how to estimate probability of Jack,given built? Okay, so we can count all different bigrams here, like that Jack, that lay, etc., and say that only four of them are that Jack. It means that the probability should be 4 divided by 10. So what's next? We can count some probabilities. We can estimate them from data. Well, why do we need this? How can we use this? Actually, we need this everywhere. So to begin with,let's discuss this Smart Reply technology. This is a technology by Google. You can get some email, and it tries to suggest some automatic reply. So for example, it can suggest that you should say thank you. How does this happen? Well, this is some text generation, right? This is some language model. And we will speak about this later,in many, many details, during week four. So also, there are some other applications, like machine translation or speech recognition. In all of these applications, you try to generate some text from some other data. It means that you want to evaluate probabilities of text, probabilities of long sequences. Like here, can we evaluate the probability of This is the house, or the probability of a long,long sequence of 100 words? Well, it can be complicated because maybe the whole sequence never occurs in the data. So we can count something, but we need somehow to deal with small pieces of this sequence, right? So let's do some math to understand how to deal with small pieces of this sequence. So here, this is our sequence of keywords. And we would like to estimate this probability. And we can apply chain rule,which means that we take the probability of the first word, and then condition the next word on this word, and so on. So that's already better. But what about this last term here? It's still kind of complicated because the prefix, the condition, there is too long. So can we get rid of it? Yes, we can. So actually, Markov assumption says you shouldn't care about all the history. You should just forget it. You should just take the last n terms and condition on them, or to be correct, last n-1 terms. So this is where they introduce assumption, because not everything in the text is connected. And this is definitely very helpful for us because now we have some chance to estimate these probabilities. So here, what happens for n = 2, for bigram model? You can recognize that we already know how to estimate all those small probabilities in the right-hand side,which means we can solve our task. So for a toy corpus again,we can estimate the probabilities. And that's what we get. Is it clear for now? I hope it is. But I want you to think about if everything is nice here. Are we done?",28,Science & Technology,PT10M33S,633,2d,hd,FALSE,,1,40951,580,35,0,37
31,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,wNBaNhvL4pg,2018-07-27T12:34:34Z,27/7/18 12:34,Simple Deep Neural Networks for Text Classification,"Hi. In this video, we will apply neural networks for text. And let's first remember, what is text? You can think of it as a sequence of characters, words or anything else. And in this video, we will continue to think of text as a sequence of words or tokens. And let's remember how bag of words works. You have every word and forever distinct word that you have in your dataset, you have a feature column. And you actually effectively vectorizing each word with one-hot-encoded vector that is a huge vector of zeros that has only one non-zero value which is in the column corresponding to that particular word. So in this example, we have very, good, and movie, and all of them are vectorized independently. And in this setting, you actually for real world problems, you have like hundreds of thousands of columns. And how do we get to bag of words representation? You can actually see that we can sum up all those values, all those vectors, and we come up with a bag of words vectorization that now corresponds to very, good, movie. And so, it could be good to think about bag of words representation as a sum of sparse one-hot-encoded vectors corresponding to each particular word. Okay, let's move to neural network way. And opposite to the sparse way that we've seen in bag of words, in neural networks, we usually like dense representation. And that means that we can replace each word by a dense vector that is much shorter. It can have 300 values, and now it has any real valued items in those vectors. And an example of such vectors is word2vec embeddings, that are pretrained embeddings that are done in an unsupervised manner. And we will actually dive into details on word2vec in the next two weeks. But, all we have to know right now is that, word2vec vectors have a nice property. Words that have similar context in terms of neighboring words, they tend to have vectors that are collinear, that actually point to roughly the same direction. And that is a very nice property that we will further use. Okay, so, now we can replace each word with a dense vector of 300 real values. What do we do next? How can we come up with a feature descriptor for the whole text? Actually, we can use the same manner as we used for bag of words. We can just dig the sum of those vectors and we have a representation based on word2vec embeddings for the whole text, like very good movie. And, that's some of word2vec vectors actually works in practice. It can give you a great baseline descriptor, a baseline features for your classifier and that can actually work pretty well. Another approach is doing a neural network over these embeddings.",28,Science & Technology,PT14M47S,887,2d,hd,FALSE,https://i.ytimg.com/vi/wNBaNhvL4pg/maxresdefault.jpg,1,66710,1142,39,0,57
32,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,h-Tpb_blwb0,2018-07-25T12:09:46Z,25/7/18 12:09,NLP - Linear Models for Text Sentiment Analysis,"In this video, we will talk about first text classification model on top of features that we have described. And let's continue with the sentiment classification. We can actually take the IMDB movie reviews dataset, that you can download, it is freely available. It contains 25,000 positive and 25,000 negative reviews. And how did that dataset appear? You can actually look at IMDB website and you can see that people write reviews there, and they actually also provide the number of stars from one star to ten star. They actually rate the movie and write the review. And if you take all those reviews from IMDB website, you can actually use that as a dataset for text classification because you have a text and you have a number of stars, and you can actually think of stars as sentiment. If we have at least seven stars, you can label it as positive sentiment. If it has at most four stars, that means that is a bad movie for a particular person and that is a negative sentiment. And that's how you get the dataset for sentiment classification for free. It contains at most 30 reviews per movie just to make it less biased for any particular movie. These dataset also provides a 50/50 train test split so that future researchers can use the same split and reproduce their results and enhance the model. For evaluation, you can use accuracy and that actually happens because we have the same number of positive and negative reviews. So our dataset is balanced in terms of the size of the classes so we can evaluate accuracy here. Okay, so let's start with first model. Let's takes features, let's take bag 1-grams with TF-IDF values. And in the result, we will have a matrix of features, 25,000 rows and 75,000 columns, and that is a pretty huge feature matrix. And what is more, it is extremely sparse. If you look at how many 0s are there, then you will see that 99.8% of all values in that matrix are 0s. So that actually applies some restrictions on the models that we can use on top of these features. And the model that is usable for these features is logistic regression, which works like the following. It tries to predict the probability of a review being a positive one given the features that we gave that model for that particular review. And the features that we use, let me remind you, is the vector of TF-IDF values. And what you actually can do is you can find the weight for every feature of that bag of force representation. You can multiply each value, each TF-IDF value by that weight, sum all of that things and pass it through a sigmoid activation function and that's how you get logistic regression model. And it's actually a linear classification model and what's good about that is since it's linear, it can handle sparse data. It's really fast to train and what's more, the weights that we get after the training can be interpreted. And let's look at that sigmoid graph at the bottom of the slide. If you have a linear combination that is close to 0, that means that sigmoid will output 0.5. So the probability of a review being positive is 0.5. So we really don't know whether it's positive or negative. But if that linear combination in the argument of our sigmoid function starts to become more and more positive, so it goes further away from zero. Then you see that the probability of a review being positive actually grows really fast. And that means that if we get the weight of our features that are positive, then those weights will likely correspond to the words that a positive. And if you take negative weights, they will correspond to the words that are negative like disgusting or awful.",28,Science & Technology,PT10M41S,641,2d,hd,FALSE,,1,14095,207,10,0,3
33,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,7YacOe4XwhY,2018-07-23T13:37:21Z,23/7/18 13:37,Feature Extraction from Text (USING PYTHON),"Hi. In this lecture will transform tokens into features. And the best way to do that is Bag of Words. Let's count occurrences of a particular token in our text. The motivation is the following. We're actually looking for marker words like excellent or disappointed, and we want to detect those words, and make decisions based on absence or presence of that particular word, and how it might work. Let's take an example of three reviews like a good movie, not a good movie, did not like. Let's take all the possible words or tokens that we have in our documents. And for each such token, let's introduce a new feature or column that will correspond to that particular word. So, that is a pretty huge metrics of numbers, and how we translate our text into a vector in that metrics or row in that metrics. So, let's take for example good movie review. We have the word good, which is present in our text. So we put one in the column that corresponds to that word, then comes word movie, and we put one in the second column just to show that that word is actually seen in our text. We don't have any other words, so all the rest are zeroes. And that is a really long vector which is sparse in a sense that it has a lot of zeroes. And for not a good movie, it will have four ones, and all the rest of zeroes and so forth. This process is called text vectorization, because we actually replace the text with a huge vector of numbers, and each dimension of that vector corresponds to a certain token in our database. You can actually see that it has some problems. The first one is that we lose word order, because we can actually shuffle over words, and the representation on the right will stay the same. And that's why it's called bag of words, because it's a bag they're not ordered, and so they can come up in any order. And different problem is that counters are not normalized. Let's solve these two problems, and let's start with preserving some ordering. So how can we do that? Actually you can easily come to an idea that you should look at token pairs, triplets, or different combinations. These approach is also called as extracting n-grams. One gram stands for tokens, two gram stands for a token pair and so forth. So let's look how it might work. We have the same three reviews, and now we don't only have columns that correspond to tokens, but we have also columns that correspond to let's say token pairs. And our good movie review now translates into vector, which has one in a column corresponding to that token pair good movie, for movie for good and so forth. So, this way, we preserve some local word order, and we hope that that will help us to analyze this text better. The problems are obvious though. This representation can have too many features, because let's say you have 100,000 words in your database, and if you try to take the pairs of those words, then you can actually come up with a huge number that can exponentially grow with the number of consecutive words that you want to analyze. So that is a problem. And to overcome that problem, we can actually remove some n-grams. Let's remove n-grams from features based on their occurrence frequency in documents of our corpus. You can actually see that for high frequency n-grams, as well as for low frequency n-grams, we can show why we don't need those n-grams. For high frequency, if you take a text and take high frequency n-grams that is seen in almost all of the documents, and for English language that would be articles, and preposition, and stuff like that. Because they're just there for grammatical structure and they don't have much meaning. These are called stop-words, they won't help us to discriminate texts, and we can pretty easily remove them. Another story is low frequency n-grams, and if you look at low frequency n-grams, you actually find typos because people type with mistakes, or rare n-grams that's usually not seen in any other reviews. And both of them are bad for our model, because if we don't remove these tokens, then very likely we will overfeed, because that would be a very good feature for our future classifier that can just see that, okay, we have a review that has a typo, and we had only like two of those reviews, which had those typo, and it's pretty clear whether it's positive or negative. So, it can learn some independences that are actually not there and we don't really need them. And the last one is medium frequency n-grams, and those are really good n-grams, because they contain n-grams that are not stop-words, that are not typos and we actually look at them. And, the problem is there're a lot of medium frequency n-grams. And it proved to be useful to look at n-gram frequency in our corpus for filtering out bad n-grams. What if we can use the same frequency for ranking of medium frequency n-grams?",28,Science & Technology,PT14M24S,864,2d,hd,FALSE,https://i.ytimg.com/vi/7YacOe4XwhY/maxresdefault.jpg,1,44646,755,9,0,26
34,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,nxhCyeRR75Q,2018-07-18T23:22:19Z,18/7/18 23:22,NLP - Text Preprocessing and Text Classification (using Python),"Hi! My name is Andre and this week, we will focus on text classification problem. Although, the methods that we will overview can be applied to text regression as well, but that will be easier to keep in mind text classification problem. And for the example of such problem, we can take sentiment analysis. That is the problem when you have a text of review as an input, and as an output, you have to produce the class of sentiment. For example, it could be two classes like positive and negative. It could be more fine grained like positive, somewhat positive, neutral, somewhat negative, and negative, and so forth. And the example of positive review is the following. ""The hotel is really beautiful. Very nice and helpful service at the front desk."" So we read that and we understand that is a positive review. As for the negative review, ""We had problems to get the Wi-Fi working. The pool area was occupied with young party animals, so the area wasn't fun for us."" So, it's easy for us to read this text and to understand whether it has positive or negative sentiment but for computer that is much more difficult. And we'll first start with text preprocessing. And the first thing we have to ask ourselves, is what is text? You can think of text as a sequence, and it can be a sequence of different things. It can be a sequence of characters, that is a very low level representation of text. You can think of it as a sequence of words or maybe more high level features like, phrases like, ""I don't really like"", that could be a phrase, or a named entity like, the history of museum or the museum of history. And, it could be like bigger chunks like sentences or paragraphs and so forth. Let's start with words and let's denote what word is. It seems natural to think of a text as a sequence of words and you can think of a word as a meaningful sequence of characters. So, it has some meaning and it is usually like,if we take English language for example,it is usually easy to find the boundaries of words because in English we can split upa sentence by spaces or punctuation and all that is left are words.Let's look at the example,Friends, Romans, Countrymen, lend me your ears;so it has commas,it has a semicolon and it has spaces.And if we split them those,then we will get words that are ready for further analysis like Friends,Romans, Countrymen, and so forth.It could be more difficult in German,because in German, there are compound words which are written without spaces at all.And, the longest word that is still in use is the following,you can see it on the slide and it actually stands forinsurance companies which provide legal protection.So for the analysis of this text,it could be beneficial to split that compound word intoseparate words because every one of them actually makes sense.They're just written in such form that they don't have spaces.The Japanese language is a different story.",28,Science & Technology,PT14M31S,871,2d,hd,FALSE,https://i.ytimg.com/vi/nxhCyeRR75Q/maxresdefault.jpg,1,37456,501,14,0,13
35,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,FVUiPk_wwoU,2018-07-18T14:51:20Z,18/7/18 14:51,Interpreting the Fitted Line in Simple Linear Regression,"Okay, so that represented a kind of high level overview about this module, as well as, other aspects that we're going to touch upon in this course. But now let's delve into a specific case of simple linear regression and talk about what this means. So going back to our flowchart, what we're gonna talk about now is specifically the machine learning model. So that's that highlighted green box and everything else is grayed out so you can forget about everything else for now. We're just talking about our model and what form it takes. So our simple linear regression model is just that. It's very simple. We're assuming we have just one input, which in this case is, square feet of the house and one output which is the house sales price and we're just gonna fit a line,. A very simple function here not that quadratic function or higher order polynomials we talked about before, just a very simple line. And what's the equation of a line? Well, it's just intercept plus slope times our variable of interest so that we're gonna say that's wo + w1x. And what this regression model then specifies is that each one of our observations yi is simply that function evaluated at xi. So that's w0 plus w1xI plus the error term which we called epsilon i. So this is our regression model, and to be clear, this error, epsilon i, is the distance from our specific observation back down to the line. Okay, so the parameters of this model Are w0 and w1 are intercept and slope and we call these the regression coefficients. So that summarizes our simple linear regression model. Very straight forward. Very simple. But we'll get to more complicated things later.",28,Science & Technology,PT21M19S,1279,2d,hd,FALSE,https://i.ytimg.com/vi/FVUiPk_wwoU/maxresdefault.jpg,1,294,6,0,0,0
36,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,xsTmUtwUg9Q,2018-07-06T17:29:07Z,6/7/18 17:29,Simple Linear Regression by Emily Fox,"This is one of the best courses about Linear Regression! Emily Fox does a great job and explains Linear Regression in the simplest possible way. In the below, you read about the course's program in detail: In the Foundations course, we talked about how machine learning is about deriving intelligence from data. And in this course, the machine learning method that we're gonna focus on is regression. And in particular, what regression is gonna assume is that we have some features that are derived from our data that are the input to our regression model. And then our goal will be to predict some continuous valued output or response to the input. The way we're gonna do this is by learning a relationship between our inputs x and this output y. For example, maybe you're interested in how taking this machine learning specialization is going to pay off for you in the end. So you're sitting here, you're doing a lot of really hard work and you wonder where this is going to land you. Well, maybe a question you might be interested in is what will your salary be after taking this specialization. And so we can think about predicting what your salary is based on things like what your performance was in the various courses, the quality of your capstone project, how many forum responses you are participating in, and different features like this. So this would be the input to the regression model and the prediction, the output that we're trying to predict would be your expected salary at the end of this specialization. Another example is predicting the price of a stock. And to form this prediction maybe we expect that this would depend on the past history of the stock, as well as perhaps recent news events, in addition to the trends in other related commodities. Or maybe you tweeted something, and you wanna know how many people are gonna retweet what you tweeted. Well this might depend on how many followers you have, how many followers your followers have, local structure of your follower network, what hash tags you used, how many retweets you've had in the past, and other features like this. Another example that we're going to talk about in this course is a really cool example of reading your mind. Where you go and you get some kind of brain scan, could be FMRI or MEG and for our sake we're just going to think of it as producing an image of your brain even though the truth is it produces something more complicated. But we can think of all the different pixel intensities as inputs to a regression model where the goal of the output is to predict whether you felt happy or sad in response to something you were shown when you were getting that brain scan. So it's reading your mind because we want to guess how you're feeling just from an image of your brain. But in this course, we're gonna focus in on a case study of predicting house prices. So in particular, a question we're gonna ask is, what's the value of a given house? Maybe you wanna sell your house and you wanna figure out how much to list that house for. And so we're gonna derive this intelligence by looking at some data. And the data we're gonna look at include other house sales. So we're gonna have the sales price associated with a bunch of other houses, as well as the house attributes of these other houses, and from these inputs, the house attributes, we're gonna learn this relationship between house attributes and the output, which is the sales price, and use this learned model in order to make the prediction of the value of your house. And this course is all about how to form this relationship between the input and the output.",28,Science & Technology,PT16M46S,1006,2d,hd,FALSE,https://i.ytimg.com/vi/xsTmUtwUg9Q/maxresdefault.jpg,1,1506,28,0,0,1
37,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,rg4tDdAleSE,2018-07-04T15:17:12Z,4/7/18 15:17,Practical Guide to Logistic Regression Analysis in R,"Logistic regression is a method for fitting a regression curve, y = f(x) when y is a categorical variable. The typical use of this model is predicting y given a set of predictors x. The predictors can be continuous, categorical or a mix of both. In this video, we will show how to perform Logistic Regression in R!",28,Science & Technology,PT9M40S,580,2d,hd,FALSE,https://i.ytimg.com/vi/rg4tDdAleSE/maxresdefault.jpg,1,262,3,2,0,2
38,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,8dvmk49jSNE,2018-07-02T13:38:00Z,2/7/18 13:38,Decision Tree Classifier implementation in R,,28,Science & Technology,PT10M25S,625,2d,hd,FALSE,https://i.ytimg.com/vi/8dvmk49jSNE/maxresdefault.jpg,1,515,3,0,0,0
39,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,0G6gW-w8ZQo,2018-06-26T20:20:03Z,26/6/18 20:20,Random Forest Using R: Step by Step Tutorial,"You can download the ""Credit Card Dataset"" from the below link: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients Learn Data Science & Machine Learning by doing! Hands On Experience Data Scientist has been ranked the number one job on Glassdoor and the average salary of a data scientist is over $120,000 in the United States according to Indeed! Data Science is a rewarding career that allows you to solve some of the world's most interesting problems! This course is designed for both complete beginners with no programming experience or experienced developers looking to make the jump to Data Science! This course is for those : 1. Who wants to be Data Scientist 2. Who are working as analyst / software developer but wants to be Data Scientist What is Data Science ? Data science is used to extract patterns or insights from data to predict future or to understand customer behavior and so on. Data science is a ""concept to unify statistics, data analysis and their related methods"" in order to ""understand and analyze actual phenomena"" with data Mining large amounts of structured and unstructured data to identify patterns can help an organization to reduce costs, increase efficiencies, recognize new market opportunities and increase the organization's competitive advantage. Some Data Science and machine learning Applications Netflix uses data science & machine learning to mine movie viewing patterns to understand what drives user interest, and uses that to make decisions on which Netflix original series to produce. Companies like Flipkart and Amazon uses data science and machine learning to understand the customer shopping behavior to do better recommendations. Gmail's spam filter uses data science (machine learning algorithm) to process incoming mail and determines if a message is junk or not.. Proctor & Gamble utilizes data science (machine learning ) models to more clearly understand future demand, which help plan for production levels more optimally. Why Programming Won't Work in some Cases?? Have you ever thought of the scenario where all the cars will be moving without a driver that means something like automated machines say for example automatic washing machine. But there is a difference. 1. For automatic washing machine,we can write programs for the washing machine functionality. 2. For automated cars without drivers in high traffic.Just imagine ,how complex and dangerous it will be when someone starts coding /programming for such functionalities.For cars to automate we would require something which is called ""Machine Learning "" In this course, we are first going to first discuss Data Structures,etc. in R like : 1. Vectors 2. Matrices 3. Data Frames 4. Factors 5. Numerical/Categorical Variables 6. List 7. How to convert matrix into data frame Programming in R Data Visualization Then implementation/working of machine learning models like 1. Linear Regression 2. Decision Tree 3. Random Forest 4.Neural Networks 5. Deep learning 6. H2o framework 7. Cross validation /How to avoid Over fitting 8. Dimensionality Reduction Techniques All the materials for this data science & machine learning course are FREE. You can download and install R, with simple commands on Windows, Linux, or Mac. This course focuses on ""how to build and understand"", not just ""how to use"".It's not about ""remembering facts"", it's about ""seeing for yourself"" via experimentation. It will teach you how to visualize what's happening in the model internally.",28,Science & Technology,PT32M52S,1972,2d,hd,FALSE,,1,7467,16,8,0,1
40,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,tO6hTI8CXaM,2018-05-13T23:29:08Z,13/5/18 23:29,Deep Reinforcement Learning Essential Prerequisite Review,In this section we are going to review all the background knowledge you need to have in order to understand Deep Reinforcement Learning. This includes: ** Markov Decision Processes (MDPs) ** Dynamic Programming ** Monte Carlo ** Temporal difference learning ** Deep Learning ** Approximation Methods ** State Transition Probabilities Hope to enjoy it!,28,Science & Technology,PT31M13S,1873,2d,hd,FALSE,,1,718,16,0,0,0
41,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,CHfrCEflVxE,2018-05-12T20:11:29Z,12/5/18 20:11,Deep Reinforcement Learning in Python - Introduction,"The Complete Guide to Mastering Artificial Intelligence using Deep Learning and Neural Networks! Requirements: • Know reinforcement learning basics, MDPs, Dynamic Programming, Monte Carlo, TD Learning • Calculus and probability at the undergraduate level • Experience building machine learning models in Python and Numpy • Know how to build a feedforward, convolutional, and recurrent neural network using Theano and Tensorflow This course is all about the application of deep learning and neural networks to reinforcement learning. If you’ve taken my first reinforcement learning class, then you know that reinforcement learning is on the bleeding edge of what we can do with AI. Specifically, the combination of deep learning with reinforcement learning has led to AlphaGo beating a world champion in the strategy game Go, it has led to self-driving cars, and it has led to machines that can play video games at a superhuman level. Reinforcement learning has been around since the 70s but none of this has been possible until now. The world is changing at a very fast pace. The state of California is changing their regulations so that self-driving car companies can test their cars without a human in the car to supervise. We’ve seen that reinforcement learning is an entirely different kind of machine learning than supervised and unsupervised learning. Supervised and unsupervised machine learning algorithms are for analyzing and making predictions about data, whereas reinforcement learning is about training an agent to interact with an environment and maximize its reward. Unlike supervised and unsupervised learning algorithms, reinforcement learning agents have an impetus - they want to reach a goal. This is such a fascinating perspective, it can even make supervised / unsupervised machine learning and ""data science"" seem boring in hindsight. Why train a neural network to learn about the data in a database, when you can train a neural network to interact with the real-world? While deep reinforcement learning and AI has a lot of potential, it also carries with it huge risk.",28,Science & Technology,PT9M58S,598,2d,hd,FALSE,https://i.ytimg.com/vi/CHfrCEflVxE/maxresdefault.jpg,1,1766,9,0,0,0
42,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,FP6hwIzsoCg,2018-04-06T16:15:54Z,6/4/18 16:15,Deep Learning for Handwritten Digit Recognition- Part 2,,28,Science & Technology,PT17M38S,1058,2d,hd,FALSE,https://i.ytimg.com/vi/FP6hwIzsoCg/maxresdefault.jpg,1,18991,186,11,0,40
43,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,lbFEZAXzk0g,2018-03-15T20:06:55Z,15/3/18 20:06,Deep Learning for Handwritten Digit Recognition - Part1,,28,Science & Technology,PT14M30S,870,2d,hd,FALSE,,1,57659,700,39,0,75
44,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,MYrJx-xf2jY,2018-02-19T19:25:25Z,19/2/18 19:25,What the F**k is Computer Vision?!,,28,Science & Technology,PT18M10S,1090,2d,hd,FALSE,,1,406,8,1,0,3
45,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,FZBmO8Ld8H0,2018-02-07T15:12:32Z,7/2/18 15:12,Applications of Deep Learning in Computer Vision - Artificial Neural Networks,"In this video you are going to learn about Artificial Neural Networks (ANN) and Perceptrons. Then, we will use ANN in form of a Deep Learning algorithm to use in Computer Vision",28,Science & Technology,PT11M19S,679,2d,hd,FALSE,https://i.ytimg.com/vi/FZBmO8Ld8H0/maxresdefault.jpg,1,417,9,1,0,5
46,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,R3nLFT-lSVg,2018-02-07T14:02:35Z,7/2/18 14:02,Applications of Deep Learning in Computer Vision - An Introduction to the New Course,This is an introduction to the new course we are going to publish for machine learning lovers. In this new series you are going to learn about the application of Deep Learning in Computer Vision. I hope you enjoy it!,28,Science & Technology,PT1M53S,113,2d,hd,FALSE,https://i.ytimg.com/vi/R3nLFT-lSVg/maxresdefault.jpg,1,1050,8,0,0,0
47,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,rrOgPiqYu6s,2018-01-06T13:33:21Z,6/1/18 13:33,Convolutional Weights as Image Features ( Deep Learning with TensorFlow),*** Dropouts During Testing *** Final Model Accuracy *** Convolutional Weights as Image Features,28,Science & Technology,PT4M56S,296,2d,hd,FALSE,,1,1910,6,0,0,0
48,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,rKfpEcA6hqQ,2017-12-17T14:56:28Z,17/12/17 14:56,BREAKING NEWS: The Real Video of Navy F/A-18 Encounter with UFO,A video shows an encounter between a Navy F/A-18 Super Hornet and an unknown object. It was released by the Defense Department’s Advanced Aerospace Threat Identification Program.,28,Science & Technology,PT35S,35,2d,hd,FALSE,https://i.ytimg.com/vi/rKfpEcA6hqQ/maxresdefault.jpg,1,11484,29,6,0,10
49,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,RJf98lxpFyc,2017-11-21T16:03:07Z,21/11/17 16:03,A Deeper Convolutional Neural Network with TensorFlow,In this lesson you are going to learn: ** Adding a convolutional layer to the model ** Transitioning to a dense layer ** Implementing dropout training,28,Science & Technology,PT4M9S,249,2d,hd,FALSE,https://i.ytimg.com/vi/RJf98lxpFyc/maxresdefault.jpg,1,516,7,1,0,0
50,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,M1JH4pSiG00,2017-10-16T13:41:06Z,16/10/17 13:41,Deep Convolutional Neural Network with TensorFlow,** Setting up input and weights for convolution ** Adding convolution and pooling layers ** Evaluating complete convolutional neural network,28,Science & Technology,PT6M29S,389,2d,hd,FALSE,https://i.ytimg.com/vi/M1JH4pSiG00/maxresdefault.jpg,1,654,12,1,0,0
51,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,YFoOhmNZiSw,2017-09-20T17:46:11Z,20/9/17 17:46,Pooling Layer Application: Convolutional Neural Network with TensorFlow,In this video you are going to learn: ** Implementing max pooling in TensorFlow ** Flattening the output of a pooling layer ** Visually inspecting pooling output Hope to enjoy it!,28,Science & Technology,PT4M18S,258,2d,hd,FALSE,,1,3315,21,7,0,0
52,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,90FI9hlFYLI,2017-08-25T19:14:45Z,25/8/17 19:14,Building Deep Convolutional Neural Network (CNN) with TensorFlow- Part 2 ( Understanding CNN),** Understanding input shapes for TensorFlow ** Implementing the convolutional layer ** Verifying the convolution on an example,28,Science & Technology,PT6M56S,416,2d,hd,FALSE,,1,1888,15,0,0,4
53,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,FTNNfba5CJw,2017-08-09T19:16:44Z,9/8/17 19:16,Building Deep Convolutional Neural Network (CNN) with TensorFlow- Part 1 ( Understanding CNN),** Understanding Convolutional Neural Network as a Sliding Winwod ** Max Pooling Layers with Example ** Convolutional Nets Applied to Font Problem,28,Science & Technology,PT5M4S,304,2d,hd,FALSE,,1,3381,34,2,0,1
54,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,oXXh01SwLrs,2017-07-21T13:08:31Z,21/7/17 13:08,Building Deep Multiple Hidden Layer Neural Network with TensorFlow,In this lesson you will learn how to: -- Build a deep neural network -- Choose number of layers and neurons -- Training a deep densely connected net Hope you all enjoy it!,28,Science & Technology,PT5M23S,323,2d,hd,FALSE,https://i.ytimg.com/vi/oXXh01SwLrs/maxresdefault.jpg,1,5458,40,3,0,1
55,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,ReSVIkljYj4,2017-07-11T15:53:23Z,11/7/17 15:53,Implementing Single Hidden Layer Neural Network with TensorFlow,"In this video you are going to learn: - Implementing a Neural Network with Single Hidden Layer in Tensor Flow - Intuition of ""Back-propagation"" for model training - And training your first Neural Network Hope you guys enjoy it!",28,Science & Technology,PT5M6S,306,2d,hd,FALSE,https://i.ytimg.com/vi/ReSVIkljYj4/maxresdefault.jpg,1,4391,16,2,0,2
56,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,rzilPq7xXC4,2017-07-06T15:03:15Z,6/7/17 15:03,Basic Neural Nets with TensorFlow,Hope you all enjoy this video!,28,Science & Technology,PT5M17S,317,2d,hd,FALSE,,1,976,11,0,0,0
57,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,Tb0JcgRX5Go,2017-06-29T17:39:47Z,29/6/17 17:39,Logistic Regression Training : Deep Learning with TensorFlow,In this video you will learn how to train Logistic Regression in TensorFlow. Hope you guys enjoy it! :),28,Science & Technology,PT4M54S,294,2d,hd,FALSE,https://i.ytimg.com/vi/Tb0JcgRX5Go/maxresdefault.jpg,1,779,5,0,0,1
58,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,GDOA_VM8G5o,2017-06-17T18:57:00Z,17/6/17 18:57,Logistic Regression Model Building with TensorFlow,In this tutorial you will learn about building a logistic regression learning model using TensorFlow.,28,Science & Technology,PT6M59S,419,2d,hd,FALSE,https://i.ytimg.com/vi/GDOA_VM8G5o/maxresdefault.jpg,1,2212,6,4,0,8
59,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,vLdPACYiMHY,2017-05-24T21:25:20Z,24/5/17 21:25,TensorFlow Basic Operations: Deep Learning with TensorFlow,"In this video we are going to learn about the basic Tensor operations, we are going to build a TensorFlow graph, and also we will see how to fetch and feed intermediate computations. Enjoy it!",28,Science & Technology,PT5M32S,332,2d,hd,FALSE,https://i.ytimg.com/vi/vLdPACYiMHY/maxresdefault.jpg,1,1321,15,2,0,3
60,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,b5xsuSfe0AY,2017-05-13T17:26:59Z,13/5/17 17:26,Installing TensorFlow - Deep Learning with TensorFlow,In this lesson you are going to learn how to install the Tensor Flow and warm up to get started Deep Learning programming using TensorFlow!,28,Science & Technology,PT5M34S,334,2d,hd,FALSE,https://i.ytimg.com/vi/b5xsuSfe0AY/maxresdefault.jpg,1,2492,14,4,0,2
61,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,oml-73YipmI,2017-05-01T13:03:51Z,1/5/17 13:03,Creating Recurrent Layers in Theano and Keras - Deep Learning with Python,,28,Science & Technology,PT6M29S,389,2d,hd,FALSE,https://i.ytimg.com/vi/oml-73YipmI/maxresdefault.jpg,1,1574,10,0,0,3
62,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,488BKXyDQWU,2017-04-20T19:20:30Z,20/4/17 19:20,Deep Learning for Automatic Image Captioning (Using Python)!,,28,Science & Technology,PT4M41S,281,2d,hd,FALSE,https://i.ytimg.com/vi/488BKXyDQWU/maxresdefault.jpg,1,5191,68,2,0,0
63,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,8ZWMQcd7KSo,2017-04-09T16:27:50Z,9/4/17 16:27,Reusing Pre trained Models - Deep Learning with Python,,28,Science & Technology,PT7M23S,443,2d,hd,FALSE,https://i.ytimg.com/vi/8ZWMQcd7KSo/maxresdefault.jpg,1,11378,54,2,0,9
64,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,CcabLpr2qmE,2017-03-29T14:27:32Z,29/3/17 14:27,For vs Scan loop in Theano (Deep Learning using Python),"The scan functions provides the basic functionality needed to do loops in Theano. Scan comes with many whistles and bells, which we will introduce by way of examples.",28,Science & Technology,PT5M19S,319,2d,hd,FALSE,https://i.ytimg.com/vi/CcabLpr2qmE/maxresdefault.jpg,1,455,1,0,0,0
65,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,YkGieAgSWho,2017-03-24T17:44:04Z,24/3/17 17:44,Loading Pre trained Models with Theano - Deep Learning with Python,,28,Science & Technology,PT5M16S,316,2d,hd,FALSE,,1,790,1,0,0,0
66,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,09Q4pXQUk1c,2017-03-20T21:25:39Z,20/3/17 21:25,Large Scale Datasets and Very Deep Neural Networks - Deep Learning with Python,,28,Science & Technology,PT5M18S,318,2d,hd,FALSE,,1,698,3,0,0,0
67,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,49IOTCzoWQg,2017-03-16T18:20:54Z,16/3/17 18:20,Fully Connected or Dense Layers - Deep Learning with Python,,28,Science & Technology,PT4M47S,287,2d,hd,FALSE,,1,7602,29,4,0,0
68,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,q0hLRqBB2YY,2017-03-07T13:22:11Z,7/3/17 13:22,Value of Perfect Information - Stanford University,"Probabilistic graphical models (PGMs) are a rich framework for encoding probability distributions over complex domains: joint (multivariate) distributions over large numbers of random variables that interact with each other. These representations sit at the intersection of statistics and computer science, relying on concepts from probability theory, graph algorithms, machine learning, and more. They are the basis for the state-of-the-art methods in a wide variety of applications, such as medical diagnosis, image understanding, speech recognition, natural language processing, and many, many more. They are also a foundational tool in formulating many machine learning problems. This course is the first in a sequence of three. It describes the two basic PGM representations: Bayesian Networks, which rely on a directed graph; and Markov networks, which use an undirected graph. The course discusses both the theoretical properties of these representations as well as their use in practice. The (highly recommended) honors track contains several hands-on assignments on how to represent some real-world problems. The course also presents some important extensions beyond the basic PGM representation, which allow more complex models to be encoded compactly.",28,Science & Technology,PT17M15S,1035,2d,sd,FALSE,,1,1771,9,2,0,1
69,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,cO_ZOgH60b8,2017-03-05T17:17:41Z,5/3/17 17:17,The No Bullshit Guide to Convolutional Neural Networks and Pooling Layers in Python,"Convolutional Neural Networks (CNN) are biologically-inspired variants of MLPs. From Hubel and Wiesel’s early work on the cat’s visual cortex, we know the visual cortex contains a complex arrangement of cells. These cells are sensitive to small sub-regions of the visual field, called a receptive field. The sub-regions are tiled to cover the entire visual field. These cells act as local filters over the input space and are well-suited to exploit the strong spatially local correlation present in natural images. Additionally, two basic cell types have been identified: Simple cells respond maximally to specific edge-like patterns within their receptive field. Complex cells have larger receptive fields and are locally invariant to the exact position of the pattern. The animal visual cortex being the most powerful visual processing system in existence, it seems natural to emulate its behavior. Hence, many neurally-inspired models can be found in the literature.",28,Science & Technology,PT6M40S,400,2d,hd,FALSE,,1,1075,6,0,0,0
70,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,Jy1Fkdg6npY,2017-02-28T13:51:04Z,28/2/17 13:51,Keras Behind the Scenes - Deep Learning with Python,"Keras is a high-level neural networks library, written in Python and capable of running on top of either TensorFlow or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Use Keras if you need a deep learning library that: -Allows for easy and fast prototyping (through total modularity, minimalism, and extensibility). -Supports both convolutional networks and recurrent networks, as well as combinations of the two. -Supports arbitrary connectivity schemes (including multi-input and multi-output training). -Runs seamlessly on CPU and GPU.",28,Science & Technology,PT5M25S,325,2d,hd,FALSE,,1,1164,3,0,0,2
71,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,Ggh1APzbWv8,2017-02-27T17:44:48Z,27/2/17 17:44,Optimizing a Simple Model in Pure Theano (Deep Learning with Python),"Deep learning is currently one of the best providers of solutions regarding problems in image recognition, speech recognition, object recognition, and natural language with its increasing number of libraries that are available in Python. The aim of deep learning is to develop deep neural networks by increasing and improving the number of training layers for each network, so that a machine learns more about the data until it’s as accurate as possible. Developers can avail the techniques provided by deep learning to accomplish complex machine learning tasks, and train AI networks to develop deep levels of perceptual recognition. Deep learning is the next step to machine learning with a more advanced implementation. Currently, it’s not established as an industry standard, but is heading in that direction and brings a strong promise of being a game changer when dealing with raw unstructured data. Deep learning is currently one of the best providers of solutions regarding problems in image recognition, speech recognition, object recognition, and natural language processing. Developers can avail the benefits of building AI programs that, instead of using hand coded rules, learn from examples how to solve complicated tasks. With deep learning being used by many data scientists, deeper neural networks are evaluated for accurate results. This course takes you from basic calculus knowledge to understanding backpropagation and its application for training in neural networks for deep learning and understand automatic differentiation. Through the course, we will cover thorough training in convolutional, recurrent neural networks and build up the theory that focuses on supervised learning and integrate into your product offerings such as search, image recognition, and object processing. Also, we will examine the performance of the sentimental analysis model and will conclude with the introduction of Tensorflow. By the end of this course, you can start working with deep learning right away. This course will make you confident about its implementation in your current work as well as further research.",28,Science & Technology,PT4M41S,281,2d,hd,FALSE,,1,857,4,0,0,2
72,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,K6m-iOU3iqk,2017-02-27T17:44:48Z,27/2/17 17:44,Understanding Deep Learning with Theano,"Deep learning is currently one of the best providers of solutions regarding problems in image recognition, speech recognition, object recognition, and natural language with its increasing number of libraries that are available in Python. The aim of deep learning is to develop deep neural networks by increasing and improving the number of training layers for each network, so that a machine learns more about the data until it’s as accurate as possible. Developers can avail the techniques provided by deep learning to accomplish complex machine learning tasks, and train AI networks to develop deep levels of perceptual recognition. Deep learning is the next step to machine learning with a more advanced implementation. Currently, it’s not established as an industry standard, but is heading in that direction and brings a strong promise of being a game changer when dealing with raw unstructured data. Deep learning is currently one of the best providers of solutions regarding problems in image recognition, speech recognition, object recognition, and natural language processing. Developers can avail the benefits of building AI programs that, instead of using hand coded rules, learn from examples how to solve complicated tasks. With deep learning being used by many data scientists, deeper neural networks are evaluated for accurate results. This course takes you from basic calculus knowledge to understanding backpropagation and its application for training in neural networks for deep learning and understand automatic differentiation. Through the course, we will cover thorough training in convolutional, recurrent neural networks and build up the theory that focuses on supervised learning and integrate into your product offerings such as search, image recognition, and object processing. Also, we will examine the performance of the sentimental analysis model and will conclude with the introduction of Tensorflow. By the end of this course, you can start working with deep learning right away. This course will make you confident about its implementation in your current work as well as further research.",28,Science & Technology,PT5M5S,305,2d,hd,FALSE,,1,1497,9,0,0,0
73,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,ZYNeOwxqOkY,2017-02-23T16:52:44Z,23/2/17 16:52,Introduction to Backpropagation with Python,,28,Science & Technology,PT5M24S,324,2d,hd,FALSE,,1,8134,24,10,0,2
74,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,R7loaENZJMQ,2017-02-17T16:40:19Z,17/2/17 16:40,"Day #3 - Deep Learning ""Hello World""! Classifying the MNIST Data (Deep Learning with Python)","In this class, we are going to see how to get our first deep neural network trained with Python! Hope to Enjoy it!",28,Science & Technology,PT7M58S,478,2d,hd,FALSE,,1,2820,19,2,0,2
75,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,pHVbSZ25pvY,2017-02-16T17:25:36Z,16/2/17 17:25,Day #2 - Open Source Python Libraries for Deep Learning,"In day #2 of this course, some of the most popular deep learning libraries are presented. Hope to enjoy it!",28,Science & Technology,PT4M31S,271,2d,hd,FALSE,,1,2357,9,1,0,0
76,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,5xJY6UxFe0Y,2017-02-15T15:39:55Z,15/2/17 15:39,A Step by Step Introduction to Deep Learning with Python: Day 1 - What is Deep Learning?,"We are all surrounded by data. An enormous amount of it. It is not only stored on big computational servers all over the world, on the computers in our offices, and cell phones in our pockets and constantly being generated, but also being transferred at a high volume in the air. This picture in the below only signifies how you can imagine about it. Deep learning is good at identifying patterns. Imagine for example about the image in the below. When you look at it, you can see there are bright spots in the image, whereas a lot of spots are dark. If I was to feed this image to a deep learning image recognition kind of program it would be able to identify where the bright spots are. Hope you enjoy this series of Deep Learning course!",28,Science & Technology,PT4M9S,249,2d,hd,FALSE,,1,7251,28,6,0,1
77,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,HVbUD9aA_Ys,2017-02-13T13:27:13Z,13/2/17 13:27,Deep Features - University of Washington,"So, we've learned that deep neural networks are really cool, high accuracy tool, but they can be really hard to build and learn, and require lots and lots of data. So next, we're gonna talk about something really exciting. Which is called deep features, which allow you to build neural networks, even when you don't have a lot of data. So, if you go back to our data image classification pipeline, where we start with an image, we detected some features, or other representations, and we've had that to a simple classifier, like a linear classifier. The question here is can we somehow use the features that we learn through the neural network? Those cool ones at the corners, edges and even faces, to feed that classifier? .......",28,Science & Technology,PT6M45S,405,2d,hd,FALSE,https://i.ytimg.com/vi/HVbUD9aA_Ys/maxresdefault.jpg,1,756,8,0,0,2
78,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,wCVf6bwG8gc,2017-02-13T13:27:10Z,13/2/17 13:27,Examples of Deep Learning in Computer Vision - University of Washington,"So I showed you some examples of neural networks in computer vision and doing classification. Is there a labrador retriever in this image? But they can do quite a bit more. So, for example, we can do image parsing. So in this example, for every picture in the image, you're trying to classify it and discover regions. So in the center top image, you see a region of sky, another region of grass, and so on. And this kind of image description, or is called scene understanding, is pretty cool, and you know networks again, provided significant gains.",28,Science & Technology,PT1M31S,91,2d,hd,FALSE,https://i.ytimg.com/vi/wCVf6bwG8gc/maxresdefault.jpg,1,2320,5,0,0,1
79,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,WWpj5Qs9r18,2017-02-13T13:27:09Z,13/2/17 13:27,Challenges of Deep learning - University of Washington,"Now, neural networks provide some exciting results, however, they do come with some challenges. So, on the pro side, they really enable you to represent this non-linear complex features and they have impressive results, not just in computer vision, but in some other areas like speech recognition. So systems like Siri on the phone and others use the neural networks behind the scene, as well as some text analysis tasks. And its potential for much more impact in the wide range of areas.",28,Science & Technology,PT2M23S,143,2d,hd,FALSE,https://i.ytimg.com/vi/WWpj5Qs9r18/maxresdefault.jpg,1,411,2,0,0,1
80,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,_6scoIuPJdE,2017-02-13T13:27:09Z,13/2/17 13:27,Deep Learning Performance - University of Washington,"Deep learning is exciting because it learns these complex features of images. And as we discussed earlier. They've had tremendous impact over the recent years in a variety of computer vision applications. Let me show you a couple of early examples. So, on the top of the slide here, what you see is an example of identifying traffic signs based on neural networks. So these are a data of German traffic signs and the idea is for every image, identify what sign it is. And they were able to get 99.5% accuracy using a deep neural network, which is pretty cool. On the bottom there, you see an example that came out of some work from Google on identifying the house numbers based on what's called Street View data. This is the data that Google uses driving around cars and photographing all sorts of streets around the world. And you see the images are pretty complex, and still they're able to get 97.8% accuracy on the per character level.",28,Science & Technology,PT3M6S,186,2d,hd,FALSE,https://i.ytimg.com/vi/_6scoIuPJdE/maxresdefault.jpg,1,142,2,0,0,1
81,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,pyDHbXVaWmQ,2017-02-13T13:27:09Z,13/2/17 13:27,Application of Deep Learning to Computer Vision - University of Washington,"The first place where neural networks made a tremendous amount of difference, is in an area called computer vision, so analyzing images and videos. So let's see a few examples of how deep learning, or this big neural networks, can be applied to computer vision. So to do that, it's good to understand what image features are. So in computer vision, image features are kind of like local detectors that get combined to make a prediction. So let's say we take this particular image. Suppose that I want to predict whether this a face image or not a face image. I run the neural detector, let's say a nose detector, eye detector, another eye detector, a mouth detector, and if all of these fire, you can do it and using a little neural network, you can say this is a face, and that's our prediction.",28,Science & Technology,PT5M42S,342,2d,hd,FALSE,https://i.ytimg.com/vi/pyDHbXVaWmQ/maxresdefault.jpg,1,884,5,1,0,1
82,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,2S9j1h6ckro,2017-02-13T13:14:40Z,13/2/17 13:14,Demo of Deep Learning Model on ImageNet Data - University of Washington,"So we saw that deep learning had a tremendous part in the ImageNet competition. Which allowed them to take 1.5 minute image string deeply on your network and get amazing performance to predict one of a thousand different categories. So let's go ahead and show you a little demo of what kind of categories we're talking about and how cool the predictions were. So here's an example. It was the AlexNet frame on that ImageNet data set, which we then employed as a service that can be queried from this website. And so every time I click on an image it gets sent to that service which actually runs on a GPU, so it's fast and it comes back for prediction. So if I click on this particular image here, it gets sent to a service that actually hosts in on Amazon AWS. It comes back for prediction here. It's hidden, but when I click on it, it tells me what prediction is. So if I show you this image, it might be unclear what that image is, but if I click on it, it says parking meter, it turns out to be the right label.",28,Science & Technology,PT2M58S,178,2d,hd,FALSE,https://i.ytimg.com/vi/2S9j1h6ckro/maxresdefault.jpg,1,1340,3,1,0,1
83,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,M8qdcOxDxgA,2017-02-11T17:28:58Z,11/2/17 17:28,Making Sense of the World with Deep Learning - Facebook,"In supervised learning, each input sample is provided with a target label dur¬ing training. In this talk, I will describe how deep learning methods, which are algorithms vaguely inspired by how the brain works, can be trained to predict the label of unseen inputs for three diﬀerent applications: speech recognition, text understanding and generic object recognition in images. In the ﬁrst ap¬plication, the input is 100ms of speech and the output is a phone label of the sound, in the second case the input is a sequence of words and the output is the subsequent word, in the last case the input is an image and the output is the label of the category of the object in the image (e.g., “dog”). Although these applications are very diﬀerent from each other, the learning algorithm is very similar. These methods have yielded the most accurate prediction systems on a variety of tasks, and they have recently been deployed in several commercial systems (e.g., speech recognition on Android phones and image search on Baidu search engine, to name a few).",28,Science & Technology,PT33M15S,1995,2d,hd,FALSE,,1,308,4,0,0,0
84,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,V8qrVleGY5U,2017-02-11T17:17:33Z,11/2/17 17:17,Deep Learning for Natural Language Processing,"Machine learning is everywhere in today's NLP, but by and large machine learning amounts to numerical optimization of weights for human designed representations and features. The goal of deep learning is to explore how computers can take advantage of data to develop features and representations appropriate for complex interpretation tasks. This tutorial aims to cover the basic motivation, ideas, models and learning algorithms in deep learning for natural language processing. Recently, these methods have been shown to perform very well on various NLP tasks such as language modeling, POS tagging, named entity recognition, sentiment analysis and paraphrase detection, among others. The most attractive quality of these techniques is that they can perform well without any external hand-designed resources or time-intensive feature engineering. Despite these advantages, many researchers in NLP are not familiar with these methods. Our focus is on insight and understanding, using graphical illustrations and simple, intuitive derivations.",28,Science & Technology,PT20M7S,1207,2d,hd,FALSE,https://i.ytimg.com/vi/V8qrVleGY5U/maxresdefault.jpg,1,21667,184,11,0,1
85,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,4ACDwbpUWeE,2017-02-11T16:52:34Z,11/2/17 16:52,Types of Neural Network Architectures,"In this video we are going to describe various kinds of architectures for neural networks. What I mean by an architecture, is the way in which the neurons are connected together. By far the commonest type of architecture in practical applications is a feet forward neural network where the information comes into the input units and flows in one direction through hidden layers until each reaches the output units. A much more interesting kind architecture is a recurrent neural network in which information can flow round in cycles. These networks can remember information for a long time. They can exhibit all sorts of interesting oscillations but they are much more difficult to train in part because they are so much more complicated in what they can do. Recently, however, people have made a lot of progress in training recurrent neural networks, and they can now do some fairly impressive things. The last kind of architecture that I'll describe is a symmetrically-connected network, one in which the weights are the same in both directions between two units...",28,Science & Technology,PT7M29S,449,2d,hd,FALSE,https://i.ytimg.com/vi/4ACDwbpUWeE/maxresdefault.jpg,1,3638,38,2,0,0
86,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,dDv5PmzpSM0,2017-02-07T15:19:14Z,7/2/17 15:19,Decision Theory: Utility Functions - Stanford University,"When we talked about influence diagram we included in the influence diagram nodes that represent the agent's utility function and those utility functions, we said, indicate an agent's preferences regarding the state of the world or different aspects of the state of the world. What are these utility functions and where do they come from? utility functions are necessary for our ability to compare complex scenarios that involve uncertainty or risk. It's not difficult for a person to say that they prefer an outcome where they get four million to one where they prefer three million........",28,Science & Technology,PT18M16S,1096,2d,sd,FALSE,https://i.ytimg.com/vi/dDv5PmzpSM0/maxresdefault.jpg,1,23584,167,6,0,7
87,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,JtF5-Ji8JrQ,2017-02-07T15:19:13Z,7/2/17 15:19,Decision Theory: Maximum Expected Utility - Stanford University,"We've show how probabilistic graphical models can be used for a variety of inference tasks like computing conditional probabilities or finding the map assignment. But often the thing that you actually want to do with a probability distribution is make decisions in the world. So, for example, if you're a doctor encountering a patient, it's not just enough to figure out what disease the patient has. Ultimately you need to decide what treatment to give the patient. How do we use a probability distribution and specifically a probabilistic graphical model in order to make the decisions? ......",28,Science & Technology,PT25M58S,1558,2d,sd,FALSE,https://i.ytimg.com/vi/JtF5-Ji8JrQ/maxresdefault.jpg,1,3794,38,1,0,1
88,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,rc3YDj5GiVM,2017-01-31T15:32:51Z,31/1/17 15:32,Conditional Random Fields - Stanford University (By Daphne Koller),"One very important variant of Markov networks, that is probably at this point, more commonly used then other kinds, than anything that's not of this type is what's called a conditional random field. So a conditional random field, you can think of it as a, something that looks very much like a Markov network, but for a somewhat different purpose. So let's think about what we are trying to do here. This class of model is intended to deal with what we call task-specific prediction, that where we have a set of input variables for observed variables, X, we have a set of target variables that we're trying to predict y. And, the class of models is intended to, is designed for those cases where we always have the same types of variables is the instance variables in the same types of variables as the targets...",28,Science & Technology,PT22M23S,1343,2d,sd,FALSE,,1,64852,490,67,0,11
89,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,IzlYOX0wrz0,2017-01-31T15:17:48Z,31/1/17 15:17,General Gibbs Distribution - Stanford University,"now we're going to define a much more general notion, that is considerably more expressive than the Pairwise case. And that definition is called the Gibbs distribution. So in order to motivate the notion of a Gibbs distribution, let's look at the most expressive Markov network that we could possible define in the context of pairwise interactions. So here we have four random variables, a, b, c, d, and I've introduced all of the possible pairwise edges between them. And so the question is, that we'd like to ask ourselves is, is this good enough? So, is this fully expressive?",28,Science & Technology,PT15M53S,953,2d,sd,FALSE,,1,7200,41,5,0,3
90,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,KWNtPHVf2VQ,2017-01-31T15:17:48Z,31/1/17 15:17,Pairwise Markov Networks - Stanford University,"there's two main families of graphical models. There's those that are based on directed graphs, directed acyclic graphs and those that are based on undirected graphs. The undirected graphical models are typically called Markov networks, they're also called Markov random field. We're going to start by talking about the simplest subclass of those which is pairwise Markov networks and then we're going to generalize it.",28,Science & Technology,PT11M,660,2d,sd,FALSE,https://i.ytimg.com/vi/KWNtPHVf2VQ/maxresdefault.jpg,1,4995,76,0,0,1
91,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,THmTZ7WOkbY,2017-01-28T16:23:09Z,28/1/17 16:23,Towards Practical Machine Learning with Differential Privacy and Beyond,"Machine learning (ML) has become one of the most powerful classes of tools for artificial intelligence, personalized web services and data science problems across fields. However, the use of ML on sensitive data sets involving medical, financial and behavioral data are greatly limited due to privacy concern. In this talk, we consider the problem of statistical learning with privacy constraints. Under Vapnik's general learning setting and the formalism of differential privacy (DP), we establish simple conditions that characterizes the private learnability, which reveals a mixture of positive and negative insight. We then identify generic methods that reuse existing randomness to effectively solve private learning in practice; and discuss a weaker notion of privacy — on-avg KL-privacy — that allows for orders-of-magnitude more favorable privacy-utility tradeoff, while preserving key properties of differential privacy. Moreover, we show that On-Average KL-Privacy is **equivalent** to generalization for a large class of commonly-used tools in statistics and machine learning that sample from Gibbs distributions---a class of distributions that arises naturally from the maximum entropy principle. Finally, I will describe a few exciting future directions that use statistics/machine learning tools to advance he state-of-the-art for privacy, and use privacy (and privacy inspired techniques) to formally address the problem of p-hacking in scientific discovery.",28,Science & Technology,PT1H17S,17,2d,sd,FALSE,,1,65,2,0,0,0
92,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,TIem2hUa4jo,2017-01-25T17:09:23Z,25/1/17 17:09,Graphical Models: Overview of Template Models - Stanford University,"topic is an important extension on the language on graphical models. And it's intended to deal with the very large class of cases. Where what we'd like to do is not just write down one kind of graphical model for a particular application. But rather, come up with something that is a general purpose representation that allows us to solve multiple problems using the same exact model.....",28,Science & Technology,PT10M56S,656,2d,sd,FALSE,,1,643,5,0,0,0
93,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,fRhaGgMjgso,2017-01-25T17:09:23Z,25/1/17 17:09,Template Models: Plate Models - Stanford University,"One very common kind of repeated structure occurs when we have multiple objects of the same type. So that where we want to have, all these different of the object. It's not copies of the objects, but objects of the same type all have a similar or in fact, the same probabilistic model. for reason that we'll talk about momentarily the most, one of the most common type of such models is called the Plate Model. Let's start by modelling repetition...",28,Science & Technology,PT20M9S,1209,2d,sd,FALSE,,1,376,3,0,0,0
94,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,lecy8kEjC3Q,2017-01-25T17:09:23Z,25/1/17 17:09,Template Models: Dynamic Bayesian Networks (DBNs) - Stanford University Coursera,"There are many classes of models that that allow us to represent in a single concise representation, a template over riched models that incorporate multiple copies of the same variable and also allow us to represent multiple models within as a byproduct of a single representation. But one of the most commonly used among those is for reasoning about template models where we have a system that evolves over time...",28,Science & Technology,PT23M3S,1383,2d,sd,FALSE,,1,10435,108,1,0,1
95,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,mNSQ-prhgsw,2017-01-25T17:09:23Z,25/1/17 17:09,Template Models: Hidden Markov Models - Stanford University,One simple yet extraordinarily class of probabilistic temporal models is the class of hidden Markov models. Although these are models can be viewed as a subclass of dynamic Bayesian networks. We'll see that they have their own type of structure that makes them particularly useful for a broad range of applications...,28,Science & Technology,PT12M2S,722,2d,sd,FALSE,,1,57824,420,21,0,13
96,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,6xBU74VWEuE,2017-01-24T13:46:44Z,24/1/17 13:46,Naive Bayes Classifier - Stanford University Course,,28,Science & Technology,PT9M53S,593,2d,sd,FALSE,,1,5484,37,3,0,1
97,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,1RgkCkN6IM0,2017-01-23T13:53:54Z,23/1/17 13:53,Semantics & Factorization - Stanford University,,28,Science & Technology,PT17M21S,1041,2d,sd,FALSE,,1,2706,26,0,0,1
98,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,BLbvW6FVniU,2017-01-23T13:53:54Z,23/1/17 13:53,Reasoning Patterns - Stanford University,,28,Science & Technology,PT10M,600,2d,sd,FALSE,,1,1007,6,0,0,1
99,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,JSrNWurmyLU,2017-01-23T13:53:54Z,23/1/17 13:53,Flow of Probabilistic Influence - Stanford University,,28,Science & Technology,PT14M37S,877,2d,sd,FALSE,,1,622,5,0,0,0
100,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,MUMkrhrDmqQ,2017-01-10T21:14:55Z,10/1/17 21:14,What Never Ending Learning (NELL) Really is? - Tom Mitchell,Lecture's slide: https://drive.google.com/open?id=0B_G-8vQI2_3QeENZbVptTmY1aDA,28,Science & Technology,PT55M56S,3356,2d,sd,FALSE,,1,683,12,0,0,0
101,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,rkbW3OpWP50,2017-01-10T13:56:45Z,10/1/17 13:56,Lecture 18 320x240,,22,People & Blogs,PT1H22M44S,1364,2d,sd,FALSE,,1,23,1,0,0,0
102,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,MHd4ueNolW0,2017-01-09T22:06:58Z,9/1/17 22:06,"Reinforcement Learning 2, by Tom Mitchell",Lecture's slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/MDPs_RL_04_28_2011.pdf and https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/FinalExamStudyTopics.pdf (Final study guide),22,People & Blogs,PT1H18M30S,1110,2d,sd,FALSE,,1,268,2,0,0,0
103,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,CQZD_cyurgE,2017-01-09T22:02:10Z,9/1/17 22:02,"Reinforcement Learning I, by Tom Mitchell",Lecture's slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/MDPs_RL_04_26_2011-ann.pdf,22,People & Blogs,PT1H20M6S,1206,2d,sd,FALSE,,1,1195,10,1,0,0
104,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,4lcOx4zqb7g,2017-01-09T21:58:16Z,9/1/17 21:58,Kernel Methods and SVM's by Tom Mitchell,Lecture's slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/Kernels_SVM_04_7_2011-ann.pdf,22,People & Blogs,PT1H17M1S,1021,2d,sd,FALSE,,1,1250,1,1,0,3
105,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,LXb1aCZ6MAk,2017-01-09T21:58:16Z,9/1/17 21:58,SVM's II,Lecture's slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/Kernels_SVM2_04_12_2011-ann.pdf,22,People & Blogs,PT1H18M56S,1136,2d,sd,FALSE,,1,81,0,0,0,1
106,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,OMRlnKupsXM,2017-01-09T21:58:16Z,9/1/17 21:58,Semi-Supervised Learning by Tom Mitchell,Lecture's slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/LabUnlab-3-17-2011.pdf,22,People & Blogs,PT1H16M36S,996,2d,sd,FALSE,,1,7749,46,2,0,3
107,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,ULG6XcfpEf4,2017-01-09T21:58:16Z,9/1/17 21:58,"Machine Learning in Computational Biology, by Ziv Bar-Joseph",Lecture's slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/Ziv_CompBio.pdf,22,People & Blogs,PT1H7M55S,475,2d,sd,FALSE,,1,481,4,0,0,0
108,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,YyI-S2--BYc,2017-01-09T21:58:16Z,9/1/17 21:58,Active Learning by Burr Settles,Lecture's slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/settles-2.active.pdf,22,People & Blogs,PT1H14M50S,890,2d,sd,FALSE,,1,1297,8,2,0,2
109,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,gyJiwXx3oRM,2017-01-09T21:58:16Z,9/1/17 21:58,Neural Networks and Gradient Descent by Tom Mitchell,Lecture's slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/NNets-701-3_24_2011_ann.pdf,22,People & Blogs,PT1H16M34S,994,2d,sd,FALSE,,1,1074,3,2,0,0
110,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,kknRKIJ4ytg,2017-01-09T21:58:16Z,9/1/17 21:58,Learning Representations III by Tom Mitchell,Lecture's slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/DimensionalityReduction_04_5_2011_ann.pdf,22,People & Blogs,PT1H19M21S,1161,2d,sd,FALSE,,1,34,0,0,0,0
111,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,pWweh3XZGHk,2017-01-09T21:58:16Z,9/1/17 21:58,"Learning Representations II , Deep Beliefe Networks by Tom Mitchell",Lecture's slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/DimensionalityReduction_03_29_2011_ann.pdf,22,People & Blogs,PT1H22M44S,1364,2d,sd,FALSE,,1,49,0,0,0,0
112,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,v-BsN-zKPcI,2017-01-09T21:58:16Z,9/1/17 21:58,Computational Learning Theory by Tom Mitchell,Lecture's slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/PAC-learning3_3-15-2011_ann.pdf,22,People & Blogs,PT1H10M37S,637,2d,sd,FALSE,,1,2664,21,0,0,0
113,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,WLq45vajrBY,2017-01-09T20:26:34Z,9/1/17 20:26,PAC Learning Review by Tom Mitchell,Lecture Slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/PAC-learning1-2-24-2011-ann.pdf,22,People & Blogs,PT1H20M29S,1229,2d,sd,FALSE,,1,1657,10,1,0,2
114,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,IG4Eil4vpBs,2017-01-09T20:24:29Z,9/1/17 20:24,"Graphical models 2, by Tom Mitchell",Lecture Slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/GrMod2_2_15_2011-ann.pdf,22,People & Blogs,PT1H19M10S,1150,2d,sd,FALSE,,1,156,0,0,0,0
115,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,IPGdTJAORi0,2017-01-09T20:24:29Z,9/1/17 20:24,"Graphical models 1, by Tom Mitchell",Lecture Slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/GrMod1_2_8_2011-ann.pdf,22,People & Blogs,PT1H18M31S,1111,2d,sd,FALSE,,1,239,0,0,0,0
116,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,Qiq6CrGXEr0,2017-01-09T20:24:29Z,9/1/17 20:24,"Graphical models 3, by Tom Mitchell",Lecture Slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/GrMod3_2_17_2011-ann.pdf,22,People & Blogs,PT1H16M23S,983,2d,sd,FALSE,,1,84,0,0,0,0
117,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,TGPhb36nVCI,2017-01-09T20:24:29Z,9/1/17 20:24,"Graphical models 4, by Tom Mitchell",Lecture Slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/GrMod4_2_22_2011-ann.pdf,22,People & Blogs,PT1H17M32S,1052,2d,sd,FALSE,,1,38,0,0,0,0
118,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,XdsgZ7Yczww,2017-01-09T20:24:29Z,9/1/17 20:24,Computational Learning Theory by Tom Mitchell,Lecture Slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/PAC-learning1-2-24-2011-ann.pdf,22,People & Blogs,PT1H20M49S,1249,2d,sd,FALSE,,1,1437,10,0,0,0
119,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,pQwF_qUYYR8,2017-01-09T20:24:29Z,9/1/17 20:24,Linear Regression by Tom Mitchell,Lecture slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/GenDiscr_2_1-2011.pdf,22,People & Blogs,PT1H17M4S,1024,2d,sd,FALSE,,1,275,1,0,0,0
120,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,513yuIqYd0A,2017-01-09T20:11:35Z,9/1/17 20:11,Logistic Regression by Tom Mitchell,Lecture slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/LR_1-27-2011.pdf,22,People & Blogs,PT1H20M15S,1215,2d,sd,FALSE,,1,599,2,0,0,0
121,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,gnY4HSigW6k,2017-01-09T20:08:47Z,9/1/17 20:08,"Gaussian Bayes classifiers,",Lecture slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/GNB_1-25-2011.pdf,22,People & Blogs,PT1H19M52S,1192,2d,sd,FALSE,,1,809,2,3,0,2
122,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,qZHTxW_25rc,2017-01-09T20:05:34Z,9/1/17 20:05,Naive Bayes by Tom Mitchell,In order to get the lecture slide go to the following link: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/NBayes-1-20-2011-ann.pdf,22,People & Blogs,PT1H16M58S,1018,2d,sd,FALSE,,1,1106,4,0,0,0
123,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,XEEOV9Solpo,2017-01-09T20:04:07Z,9/1/17 20:04,Probability and Estimation by Tom Mitchell,In order to get the lecture slide go to the following link: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/MLE_MAP_1-18-11-ann.pdf,22,People & Blogs,PT1H25M23S,1523,2d,sd,FALSE,,1,253,2,0,0,0
124,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,EfNfNhWnCfs,2017-01-09T19:55:01Z,9/1/17 19:55,Intro to Machine Learning- Decision Trees By Tom Mitchell,Get the slide from the following link: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/DTreesAndOverfitting-1-11-2011_final.pdf,22,People & Blogs,PT1H19M58S,1198,2d,sd,FALSE,,1,2163,4,7,0,0
125,UChIaUcs3tho6XhyU6K6KMrw,Machine Learning TV,k-4YGQ4mZIM,2017-01-09T19:54:35Z,9/1/17 19:54,"Overfitting, Random variables and probabilities by Tom Mitchell",Get the slide from the following link: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/Overfitting_ProbReview-1-13-2011-ann.pdf,22,People & Blogs,PT1H18M28S,1108,2d,sd,FALSE,,1,363,5,0,0,0